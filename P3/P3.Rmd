---
title: "Práctica 3"
author:
    - "Laura Calle Caraballo"
    - "Javier León Palomares"
date: "21 de mayo de 2017"
lang: es
header-includes:
    - \usepackage{amsmath}
    - \usepackage{fancyhdr}
    - \usepackage[math]{iwona}
    - \usepackage{color}
    - \usepackage{array}
    - \pagestyle{fancy}
    - \fancyhead[LO]{\textcolor[rgb]{0,0,0}{Grado en Ingeniería Informática}}
    - \fancyhead[RO]{\textcolor[rgb]{0.2,0.2,0.9}{Aprendizaje Automático, Curso 2016-2017}}
output: 
    pdf_document:
        fig_caption: yes
        toc: true
        number_sections: yes
        highlight: pygments
        includes:
            after_body: P3Apendice.Rmd
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

```{r,echo=FALSE,message=FALSE}
library("caret", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("e1071", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("glmnet", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("ROCR", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("leaps", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
```

```{r,echo=FALSE}
set.seed(3)

clasificar_recta = function(punto, recta){
    
    sign(punto[2] - recta[1]*punto[1] - recta[2])
    
}

error_cuadratico = function(clasificados,reales){
    
    squared_error = function(pair_h_y){ # (h(x) - y_n)^2
        (pair_h_y[1]-pair_h_y[2])^2
    }
    
    if(min(reales) == 0){
        
        reales[reales == 0] = -1
        clasificados[clasificados < 0.5] = -1
        clasificados[clasificados >= 0.5] = 1
        
    }
    
    pos_errores = which(clasificados != reales) # Qué etiquetas ha clasificado mal.
    pares_errores = cbind(clasificados[pos_errores],reales[pos_errores],deparse.level=0)
    # Sumamos los errores al cuadrado y hacemos la media.
    sum(apply(pares_errores,1,squared_error))/length(clasificados)
}

porcentaje_error = function(clasificados,reales,fp=1,fn=1){
    
    reales[reales == 0] = -1
    t = table(clasificados,reales)
    total_predicciones = sum(t)
    t[1,2] = t[1,2]*fn
    t[2,1] = t[2,1]*fp
    100*(1-sum(diag(t))/total_predicciones)
    
}

categorizar = function(clasificados,umbral=0.5){
    
    clasificados[clasificados < umbral] = -1
    clasificados[clasificados >= umbral] = 1
    clasificados
    
}

leer_datos_spam = function(){
    
    datos = read.table("./datos/spam.data")
    conjuntos = read.table("./datos/spam.traintest")
    etiquetas = datos[,ncol(datos)]
    list(datos=datos[,-ncol(datos)],etiquetas=etiquetas,conjuntos=conjuntos)
    
}

# Evalúa la regresión para unos datos
evaluar_regresion = function(regresion,datos){
    
    predict(regresion,datos) # Los datos no deben incluir las etiquetas
    
}

# Calcula una cota para E_out en función del error E_test
calcular_cota_eout = function(N,delta) sqrt((log(delta/2))/(-2*N))
```

# Clasificación.

Usaremos la base de datos _SPAM E-mail Database_, que contiene 58 atributos distribuidos de la siguiente forma:

\begin{itemize}

    \item
    48 variables reales continuas en el intervalo $[0,100]$ que corresponden a la proporción en la que aparecen 48 palabras concretas en un correo.
    \item
    6 variables reales continuas en el intervalo $[0,100]$ que corresponden a la proporción en la que aparecen los caracteres ; · ( · [ · ! · \$ · \#.
    \item
    Una variable real continua que representa la longitud media de las secuencias de caracteres en mayúsculas.
    \item
    Una variable entera continua que contiene la longitud de la mayor cadena de caracteres encontrada en mayúsculas.
    \item
    Una variable entera que corresponde al total de mayúsculas encontradas.
    \item
    La variable de respuesta (1 si el correo se consideró spam y 0 si no se consideró spam).

\end{itemize}

Esta base de datos no contiene valores perdidos y su proporción de etiquetas es de $39,4\%$ de positivos y $60,6\%$ de negativos.

Las particiones de entrenamiento y test se encuentran ya definidas de tal forma que usaremos 3065 de los 4601 datos para ajustar los modelos y los 1536 restantes para realizar pruebas.

## Preprocesamiento de datos.

Para realizar el preprocesamiento de datos hemos empleado la función \texttt{preProcess} del paquete \texttt{caret}; esta función permite aplicar distintos métodos de transformación para adecuar las características iniciales a los modelos que vamos a emplear.

A continuación vamos a explicar los métodos utilizados:

\begin{itemize}

    \item
    Transformación de \textit{Yeo-Johnson}: es muy similar a la transformación de \textit{Box-Cox} pero, a diferencia de ésta, permite la existencia de valores negativos ó 0. En nuestro caso, no existen valores negativos pero sí es bastante frecuente encontrar valores iguales a 0.
    \item
    Centrado: se realiza la media de los valores de cada característica y se le resta a cada valor particular, siendo la nueva media igual a 0. Esto reduce las distancias entre las distintas características.
    \item
    Escalado: este método divide los valores por su desviación típica. El objetivo de esta transformación es tener características con rangos uniformes para evitar problemas con modelos que consideran distancias entre valores (lo cual favorecería las características con valores más separados entre sí); además, la independencia respecto a la escala en la que se realizaron las medidas también es importante.
    \item
    Análisis de componentes principales (\textit{PCA}): se buscan nuevos ejes de coordenadas de forma que la varianza de algunas características sea lo suficientemente pequeña como para descartarlas. Como resultado se consigue una reducción de dimensionalidad donde las nuevas características, que no guardan relación de significado con las originales, representarán la parte más significativa de la varianza de los datos.

\end{itemize}

Otro método que se podría haber utilizado para reducir la dimensionalidad de los datos es _Near Zero Variance_, que elimina predictores con varianza cercana a 0, ya que esto suele indicar que su valor es casi constante a lo largo de la muestra y apenas aportan información. En el caso de este conjunto de datos muchas de las características representan frecuencias de aparición de palabras concretas, por lo que es de esperar que en muchas observaciones estos valores sean 0; sin embargo, este hecho no implica que sus contribuciones no tengan importancia.

Una vez explicadas las transformaciones, veamos cómo se traduce en la práctica a código:

```{r,cache=TRUE}
# Lectura de datos y obtención del conjunto de entrenamiento
spam = leer_datos_spam()
indices_train = which(spam$conjuntos == 0)

# Función que encapsula el preprocesado de datos
preprocesar_datos = function(datos,indices_train,metodos,umbral_varianza){
    
    preprocess_obj = preProcess(datos[indices_train,],method=metodos,umbral_varianza)
    nuevosDatos = predict(preprocess_obj,datos)
    
}

# Preprocesamiento (Yeo-Johnson, centrado, escalado, análisis de componentes principales...)
spam_procesado = preprocesar_datos(spam$datos,indices_train,c("YeoJohnson","center","scale","pca"),0.85)
# Sin PCA
spam_procesado_sin_pca = preprocesar_datos(spam$datos,indices_train,c("YeoJohnson","center","scale"),0.85)
```
```{r,echo=FALSE,cache=TRUE}
# Añadir las etiquetas para la regresión lineal
spam_procesado = cbind(spam_procesado,spam$etiquetas)
spam_procesado_sin_pca = cbind(spam_procesado_sin_pca,spam$etiquetas)
colnames(spam_procesado)[ncol(spam_procesado)] = "etiquetas"
colnames(spam_procesado_sin_pca)[ncol(spam_procesado_sin_pca)] = "etiquetas"
```

Ya que existe la posibilidad de que el análisis de componentes principales elimine características relevantes, vamos a realizar experimentos separados preprocesando los datos con y sin esta técnica.

## Clases de funciones a utilizar.

Ya que esta práctica trata de ajustes de modelos lineales, las clases de funciones a utilizar serán, en principio, las correspondientes a polinomios de grado 1.

No obstante, si observásemos indicios de no linealidad, podríamos plantearnos realizar algún tipo de transformación no lineal utilizando, por ejemplo, los polinomios de _Legendre_.

## Regularización.

Es bastante común enfrentarnos al sobreajuste derivado de considerar una clase demasiado compleja sobre los datos de entrenamiento. Según el principio de la _navaja de Ockham_, la explicación más simple suele ser la correcta, por lo que una posible aproximación puede ser la simplificación.

La regularización toma este concepto para proponer una solución al sobreajuste. La metodología se basa en penalizar los valores altos en las componentes del vector de pesos de la función (ya sea forzando que algunos pesos sean 0 ó restringiendo los valores de todos ellos, entre otras formas).

En el caso de las funciones lineales, la regularización no parece en principio necesaria, debido a que únicamente consideraremos hiperplanos, el tipo más simple de clasificadores. Aun así, comprobaremos los efectos de aplicar esta técnica para asegurarnos que no es necesaria; en concreto, utilizaremos _weight decay_, proporcionado por el paquete \texttt{glmnet}.

## Modelos utilizados.

En primer lugar, es necesario hablar de \texttt{regsubsets}, función mediante la cual seleccionaremos los conjuntos de  características con las que construiremos distintos modelos para predecir la variable de respuesta. Dicha función proporciona, para un parámetro $n$, los conjuntos $\{C_1,C_2,\ldots,C_i,\ldots,C_n : |C_i| = i\}$ de los mejores predictores que encuentra. Esta selección de características se puede realizar de varias maneras diferentes:

\begin{itemize}

    \item
    Hacia delante: se comienza sin variables y se añade, tras probarlas todas, la que produce una mejora más significativa del ajuste. Se repite este proceso hasta que ninguna incrementa la efectividad de forma estadísticamente notable.
    \item
    Hacia atrás: se empieza contando con todas las variables y se elimina la que empeora de forma menos significativa la calidad del ajuste. El proceso se repite hasta que ninguna variable se puede eliminar sin deteriorar seriamente el modelo.
    \item
    \textit{Sequential Replacement}: una combinación de variables predictoras (modelo) se representa mediante un vector y está asociada a una medida de su calidad. En cada iteración se realizan todos los reemplazos posibles de variables originales por otras variables y se escoge la mejor nueva combinación, que será el punto de partida de la siguiente iteración. Este proceso se repite hasta que no haya mejora (convergencia).
    \item
    Exhaustiva: se utiliza \textit{Branch and Bound} para realizar una búsqueda lo más completa posible manteniendo un nivel aceptable de eficiencia.
    
\end{itemize}

En los casos en los que se haya realizado una reducción de dimensionalidad importante usaremos una búsqueda exhaustiva. Sin embargo, si tenemos un número considerable de características deberemos optar por métodos más rápidos ya que el tiempo de ejecución la convertiría en inviable; en particular, si no utilizamos análisis de componentes principales, usaremos la búsqueda hacia delante.

La función que utiliza \texttt{regsubsets} para obtener los subconjuntos de variables con los que vamos a probar en las distintas regresiones es la siguiente:

```{r}
subconjuntos_formulas = function(datos,max_tam,metodo="exhaustive"){
    # Obtenemos los subconjuntos de variables
    subsets = regsubsets(etiquetas~.,data=datos,method=metodo,nvmax=max_tam)
    # Obtenemos la matriz de características seleccionadas por grupos de tamaño [1,nvmax]
    matriz_subsets = summary(subsets)$which[,-1]
    # Guardamos, para cada fila, las columnas cuyas variables han sido seleccionadas.
    seleccionados = apply(matriz_subsets,1,which)
    # Obtenemos los nombres de esas columnas (para utilizarlos en la regresión)
    seleccionados = lapply(seleccionados,names)
    # Construimos la suma de las variables que usaremos en la regresión lineal
    seleccionados = mapply(paste,seleccionados,MoreArgs=list(collapse="+"))
    # Construimos strings equivalentes a las fórmulas que usaremos en la regresión lineal
    formulas = mapply(paste,rep("etiquetas~",max_tam),seleccionados,USE.NAMES = FALSE)
    # Construimos objetos fórmula
    formulas = apply(matrix(formulas,nrow=length(formulas)), 1, as.formula)
    list(formulas=formulas,cp=subsets$cp,bic=subsets$bic)
}
```

```{r,echo=FALSE,cache=TRUE}
# Buscamos exhaustivamente conjuntos de características que usar
max_caracteristicas = ncol(spam_procesado)-1
# Calculamos los objetos fórmula para cada subconjunto de variables
seleccion_caracteristicas = subconjuntos_formulas(spam_procesado[indices_train,],max_caracteristicas)
formulas = seleccion_caracteristicas$formulas
seleccion_caracteristicas_sin_pca = subconjuntos_formulas(spam_procesado_sin_pca[indices_train,],max_caracteristicas,metodo="forward")
formulas_sin_pca = seleccion_caracteristicas_sin_pca$formulas
```

En esta función comenzamos obteniendo una matriz que nos indica qué variables se seleccionan para cada tamaño; a continuación, extraemos los nombres de las características para posteriormente construir sus fórmulas asociadas y así poder utilizarlas como argumento en las etapas siguientes de la experimentación.

Como modelos básicos de ajuste hemos utilizado la regresión lineal con una familia de funciones gaussiana que implementa la función \texttt{lm} y la regresión lineal logística proporcionada como una de las posibilidades de la función \texttt{glm}. Asimismo, hemos considerado el uso de regularización para comprobar empíricamente si es útil en nuestro caso.

Para realizar los experimentos sobre una base común, la misma colección de conjuntos de predictores obtenida mediante \texttt{regsubsets} será empleada tanto con \texttt{lm} como con \texttt{glm}. Para ambas funciones, la decisión de qué conjunto representa el mejor modelo se regirá por distintos criterios: la proporción de etiquetas incorrectas frente al total y los estimadores _Bayesian information criterion_ (BIC) y $C_p$. 

Comenzando por la regresión lineal de la función \texttt{lm}, los únicos parámetros que necesita son los datos de entrenamiento ya procesados, sus etiquetas y las variables que va a utilizar para calcular la regresión. La función que encapsula el proceso hasta el cálculo del porcentaje de error es la siguiente:

```{r}
evalua_lm = function(formula,datos,subconjunto,fp=1,fn=1){
    # do.call pasa los parámetros a lm de forma correcta
    reg_lin = do.call("lm", list(formula=formula, data=substitute(datos),
                                 subset=substitute(subconjunto)))
    prediccion_test = evaluar_regresion(reg_lin,datos[-subconjunto,-ncol(datos)])
    porc_error = porcentaje_error(categorizar(prediccion_test),
                                  datos[-subconjunto,ncol(datos)],fp,fn)
    list(formula=formula, error = porc_error)
}
```

Para utilizar lo anterior en los datos con y sin selección de características sólo necesitamos las siguientes líneas:

```{r,cache=TRUE}
ajustes_lm = mapply(evalua_lm, formulas, MoreArgs = list(datos = spam_procesado,
                                                    subconjunto = indices_train))
ajustes_lm_sin_pca = mapply(evalua_lm, formulas_sin_pca,
                            MoreArgs = list(datos = spam_procesado_sin_pca,
                                            subconjunto = indices_train))
```

Del mismo modo, utilizaremos la función \texttt{glm} para calcular regresiones logísticas. Los parámetros requeridos son los mismos que para \texttt{lm} aunque con la inclusión de la familia de funciones que queremos utilizar, que por defecto es la gaussiana (correspondiente a la regresión lineal). Nosotros emplearemos la binomial, ya que trata con la probabilidad de etiquetar una observación en una de dos posibles categorías; en otras palabras, implementa la regresión logística. De nuevo, la función de \texttt{R} que encapsula el uso de \texttt{glm} se muestra a continuación:

```{r}
evalua_glm = function(formula,datos,subconjunto,fp=1,fn=1,familia=binomial()){
    reg_lin = do.call("glm", list(formula=formula, data=substitute(datos),
                                  subset=substitute(subconjunto),familia))
    prediccion_test = evaluar_regresion(reg_lin,datos[-subconjunto,-ncol(datos)])
    porc_error = porcentaje_error(categorizar(prediccion_test),
                                  datos[-subconjunto,ncol(datos)],fp,fn)
    list(formula=formula, error = porc_error)
}
```

Las llamadas a función para calcular las regresiones logísticas quedan, pues, de la siguiente forma:

```{r,cache=TRUE}
ajustes_glm = mapply(evalua_glm, formulas, 
                     MoreArgs = list(datos = spam_procesado, 
                                     subconjunto = indices_train))
ajustes_glm_sin_pca = mapply(evalua_glm, formulas_sin_pca, 
                             MoreArgs = list(datos = spam_procesado_sin_pca,
                                             subconjunto = indices_train))
```

Finalmente, antes de aplicar regularización debemos estudiar en qué circunstancias tiene sentido. Una vez realizado el análisis de componentes principales, el cual ya elimina presumiblemente características que conduzcan a un sobreajuste, la regularización podría provocar _underfitting_ o infraajuste. No obstante, si omitimos la etapa de PCA este problema tiene menos posibilidades de presentarse, por lo que realizaremos el experimento en este último caso.

El paquete \texttt{glmnet} implementa la funcionalidad que necesitamos. La regularización precisa de un hiperparámetro $\lambda$ que hemos de calcular previamente usando la función \texttt{cv.glmnet}, la cual aplica validación cruzada para obtener el mejor valor de $\lambda$ (el que obtiene una menor tasa de error).

```{r,cache=TRUE}
# Creamos la matriz de datos en el formato que necesita glmnet
x = model.matrix(etiquetas~.,spam_procesado_sin_pca)[,-ncol(spam_procesado_sin_pca)]
y = spam_procesado_sin_pca$etiquetas
# Obtenemos los errores de validación cruzada en el conjunto
cv.out = cv.glmnet(x[indices_train,],y[indices_train],alpha=0)
# Guardamos el lambda que ha dado menor error de validación cruzada
bestlambda = cv.out$lambda.min
```

La función \texttt{cv.glmnet} devuelve, entre otros datos, un modelo que no consideraremos ya que es resultado del proceso de validación cruzada; esto significa que ha sido ajustado con un subconjunto del conjunto de entrenamiento y posiblemente no tenga la calidad suficiente.

Por ello, vamos a hacer uso de la función \texttt{glmnet}; ésta recibe como parámetros los datos y etiquetas en el mismo formato que \texttt{cv.glmnet}, además de un $\alpha$ que varía entre 0 y 1 e indica la proporción en la que se aplican respectivamente _weight decay_ y _LASSO_, y un conjunto de valores de $\lambda$ que se encargará de probar (en nuestro caso, sólo el mejor $\lambda$ obtenido como ya hemos visto).

```{r}
# Obtenemos un modelo de Ridge
modelo_ridge = glmnet(x,y,alpha=0,lambda=bestlambda)
# Calculamos las predicciones y el error asociado a ellas
modelo_ridge.pred = predict(modelo_ridge,s=bestlambda,newx=x[-indices_train,])
error_ridge = porcentaje_error(categorizar(modelo_ridge.pred),
                               spam_procesado[-indices_train,ncol(spam_procesado)],fp=1)
```

Una vez obtenidos los modelos, es el momento de estudiar sus tasas de error y elegir el mejor de ellos; lo haremos, pues, en la siguiente sección.

## Selección del modelo final

