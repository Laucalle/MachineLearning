---
title: "Práctica 3"
author:
    - "Laura Calle Caraballo"
    - "Javier León Palomares"
date: "21 de mayo de 2017"
lang: es
header-includes:
    - \usepackage{amsmath}
    - \usepackage{fancyhdr}
    - \usepackage[math]{iwona}
    - \usepackage{color}
    - \usepackage{array}
    - \pagestyle{fancy}
    - \fancyhead[LO]{\textcolor[rgb]{0,0,0}{Grado en Ingeniería Informática}}
    - \fancyhead[RO]{\textcolor[rgb]{0.2,0.2,0.9}{Aprendizaje Automático, Curso 2016-2017}}
output: 
    pdf_document:
        fig_caption: yes
        toc: true
        number_sections: yes
        highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

```{r,echo=FALSE}
library("caret", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("e1071", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("glmnet", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("ROCR", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("leaps", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
```

```{r,echo=FALSE}
set.seed(3)

clasificar_recta = function(punto, recta){
    
    sign(punto[2] - recta[1]*punto[1] - recta[2])
    
}

error_cuadratico = function(clasificados,reales){
    
    squared_error = function(pair_h_y){ # (h(x) - y_n)^2
        (pair_h_y[1]-pair_h_y[2])^2
    }
    
    if(min(reales) == 0){
        
        reales[reales == 0] = -1
        clasificados[clasificados < 0.5] = -1
        clasificados[clasificados >= 0.5] = 1
        
    }
    
    pos_errores = which(clasificados != reales) # Qué etiquetas ha clasificado mal.
    pares_errores = cbind(clasificados[pos_errores],reales[pos_errores],deparse.level=0)
    # Sumamos los errores al cuadrado y hacemos la media.
    sum(apply(pares_errores,1,squared_error))/length(clasificados)
}

porcentaje_error = function(clasificados,reales,fp=1,fn=1){
    
    reales[reales == 0] = -1
    t = table(clasificados,reales)
    total_predicciones = sum(t)
    t[1,2] = t[1,2]*fn
    t[2,1] = t[2,1]*fp
    100*(1-sum(diag(t))/total_predicciones)
    
}

categorizar = function(clasificados,umbral=0.5){
    
    clasificados[clasificados < umbral] = -1
    clasificados[clasificados >= umbral] = 1
    clasificados
    
}

leer_datos_spam = function(){
    
    datos = read.table("./datos/spam.data")
    conjuntos = read.table("./datos/spam.traintest")
    etiquetas = datos[,ncol(datos)]
    list(datos=datos[,-ncol(datos)],etiquetas=etiquetas,conjuntos=conjuntos)
    
}

# Evalúa la regresión para unos datos
evaluar_regresion = function(regresion,datos){
    
    predict(regresion,datos) # Los datos no deben incluir las etiquetas
    
}

# Evalúa una regresión lineal dada una fórmula y unos datos de entrenamiento 
evalua_lm = function(formula,datos,subconjunto,fp=1,fn=1){
    reg_lin = do.call("lm", list(formula=formula, data=substitute(datos), subset=substitute(subconjunto)))
    prediccion_test = evaluar_regresion(reg_lin,datos[-subconjunto,-ncol(datos)])
    porc_error = porcentaje_error(categorizar(prediccion_test),datos[-subconjunto,ncol(datos)],fp,fn)
    list(formula=formula, error = porc_error)
}

# Evalúa una regresión lineal dada una fórmula, una familia y unos datos de entrenamiento 
evalua_glm = function(formula,datos,subconjunto,fp=1,fn=1,familia=binomial()){
    reg_lin = do.call("glm", list(formula=formula, data=substitute(datos), subset=substitute(subconjunto),familia))
    prediccion_test = evaluar_regresion(reg_lin,datos[-subconjunto,-ncol(datos)])
    porc_error = porcentaje_error(categorizar(prediccion_test),datos[-subconjunto,ncol(datos)],fp,fn)
    list(formula=formula, error = porc_error)
}

# Calcula una cota para E_out en función del error E_test
calcular_cota_eout = function(N,delta) sqrt((log(delta/2))/(-2*N))

spam = leer_datos_spam()
```

# Clasificación.

Usaremos la base de datos _SPAM E-mail Database_, que contiene 58 atributos distribuidos de la siguiente forma:

\begin{itemize}

    \item
    48 variables reales continuas en el intervalo $[0,100]$ que corresponden a la proporción en la que aparecen 48 palabras concretas en un correo.
    \item
    6 variables reales continuas en el intervalo $[0,100]$ que corresponden a la proporción en la que aparecen los caracteres ; · ( · [ · ! · \$ · \#.
    \item
    Una variable real continua que representa la longitud media de las secuencias de caracteres en mayúsculas.
    \item
    Una variable entera continua que contiene la longitud de la mayor cadena de caracteres encontrada en mayúsculas.
    \item
    Una variable entera que corresponde al total de mayúsculas encontradas.
    \item
    La variable de respuesta (1 si el correo se consideró spam y 0 si no se consideró spam).

\end{itemize}

Esta base de datos no contiene valores perdidos y su proporción de etiquetas es de $39,4\%$ de positivos y $60,6\%$ de negativos.

Las particiones de entrenamiento y test se encuentran ya definidas de tal forma que usaremos 3065 de los 4601 datos para ajustar los modelos y los 1536 restantes para realizar pruebas.

## Preprocesamiento de datos.

Para realizar el preprocesamiento de datos hemos empleado la función \texttt{preProcess} del paquete \texttt{caret}; esta función permite aplicar distintos métodos de transformación para adecuar las características iniciales a los modelos que vamos a emplear.

A continuación vamos a explicar los métodos utilizados:

\begin{itemize}

    \item
    Transformación de \textit{Yeo-Johnson}: es muy similar a la transformación de \textit{Box-Cox} pero, a diferencia de ésta, permite la existencia de valores negativos ó 0. En nuestro caso, no existen valores negativos pero sí es bastante frecuente encontrar valores iguales a 0.
    \item
    Centrado: se realiza la media de los valores de cada característica y se le resta a cada valor particular, siendo la nueva media igual a 0. Esto reduce las distancias entre las distintas características.
    \item
    Escalado: este método divide los valores por su desviación típica. El objetivo de esta transformación es tener características con rangos uniformes para evitar problemas con modelos que consideran distancias entre valores (lo cual favorecería las características con valores más separados entre sí); además, la independencia respecto a la escala en la que se realizaron las medidas también es importante.
    \item
    Análisis de componentes principales (\textit{PCA}): se buscan nuevos ejes de coordenadas de forma que la varianza de algunas características sea lo suficientemente pequeña como para descartarlas. Como resultado se consigue una reducción de dimensionalidad donde las nuevas características, que no guardan relación de significado con las originales, representarán la parte más significativa de la varianza de los datos.

\end{itemize}

Otro método que se podría haber utilizado para reducir la dimensionalidad de los datos es _Near Zero Variance_, que elimina predictores con varianza cercana a 0, ya que esto suele indicar que su valor es casi constante a lo largo de la muestra y apenas aportan información. En el caso de este conjunto de datos muchas de las características representan frecuencias de aparición de palabras concretas, por lo que es de esperar que en muchas observaciones estos valores sean 0; sin embargo, este hecho no implica que sus contribuciones no tengan importancia.

Una vez explicadas las transformaciones, veamos cómo se traduce en la práctica a código:

```{r}
preprocesar_datos = function(datos,metodos,umbral_varianza){
    
    preprocess_obj = preProcess(datos,method=metodos,umbral_varianza)
    nuevosDatos = predict(preprocess_obj,datos)
    
}

# Preprocesamiento (centrado, escalado, análisis de componentes principales...)
spam_procesado = preprocesar_datos(spam$datos,c("YeoJohnson","center","scale","pca"),0.85)
```

## Clases de funciones a utilizar.

Ya que esta práctica trata de ajustes de modelos lineales, las clases de funciones a utilizar serán, en principio, las correspondientes a polinomios de grado 1.

No obstante, si observásemos indicios de no linealidad, podríamos plantearnos realizar algún tipo de transformación no lineal utilizando, por ejemplo, los polinomios de _Legendre_.

## Regularización.

Es bastante común enfrentarnos al sobreajuste derivado de considerar una clase demasiado compleja sobre los datos de entrenamiento. Según el principio de la _navaja de Ockham_, la explicación más simple suele ser la correcta, por lo que una posible aproximación puede ser la simplificación.

La regularización toma este concepto para proponer una solución al sobreajuste. La metodología se basa en penalizar los valores altos en las componentes del vector de pesos de la función (ya sea forzando que algunos pesos sean 0 ó restringiendo los valores de todos ellos, entre otras formas).

En el caso de las funciones lineales, la regularización no parece en principio necesaria, debido a que únicamente consideraremos hiperplanos, el tipo más simple de clasificadores. Aun así, comprobaremos los efectos de aplicar esta técnica para demostrar que no es necesaria.

## Modelos utilizados.

En primer lugar, es necesario hablar de \texttt{regsubsets}, función mediante la cual seleccionaremos los conjuntos características con las que construiremos distintos modelos para predecir la variable de respuesta. Dicha función proporciona, para un parámetro $n$, los conjuntos $\{C_1,C_2,\ldots,C_i,\ldots,C_n : |C_i| = i\}$ de los mejores predictores que encuentra.  Esta selección de características se puede realizar de varias maneras diferentes:

\begin{itemize}

    \item
    Hacia delante: se comienza sin variables y se añade, tras probarlas todas, la que produce una mejora más significativa del ajuste. Se repite este proceso hasta que ninguna incrementa la efectividad de forma estadísticamente notable.
    \item
    Hacia atrás: se empieza contando con todas las variables y se elimina la que empeora de forma menos significativa la calidad del ajuste. El proceso se repite hasta que ninguna variable se puede eliminar sin deteriorar seriamente el modelo.
    \item
    \textit{Sequential Replacement}: una combinación de variables predictoras (modelo) se representa mediante un vector y está asociada a una medida de su calidad. En cada iteración se realizan todos los reemplazos posibles de variables originales por otras variables y se escoge la mejor nueva combinación, que será el punto de partida de la siguiente iteración. Este proceso se repite hasta que no haya mejora (convergencia).
    \item
    Exhaustiva: se utiliza \textit{Branch and Bound} para realizar una búsqueda lo más completa posible manteniendo un nivel aceptable de eficiencia.
    
\end{itemize}

En los casos en los que se haya realizado una reducción de dimensionalidad importante usaremos una búsqueda exhaustiva; sin embargo, si tenemos un número considerable de características deberemos optar por métodos más rápidos ya que el tiempo de ejecución la convertiría en inviable.

Como modelos de ajuste hemos utilizado la regresión lineal proporcionada por la función \texttt{lm} y la regresión lineal logística proporcionada como una de las posibilidades de la función \texttt{glm}.

