---
title: "Práctica 3"
author:
    - "Laura Calle Caraballo"
    - "Javier León Palomares"
date: "21 de mayo de 2017"
lang: es
header-includes:
    - \usepackage{amsmath}
    - \usepackage{fancyhdr}
    - \usepackage[math]{iwona}
    - \usepackage{color}
    - \usepackage{array}
    - \pagestyle{fancy}
    - \fancyhead[LO]{\textcolor[rgb]{0,0,0}{Grado en Ingeniería Informática}}
    - \fancyhead[RO]{\textcolor[rgb]{0.2,0.2,0.9}{Aprendizaje Automático, Curso 2016-2017}}
output: 
    pdf_document:
        fig_caption: yes
        toc: true
        number_sections: yes
        highlight: pygments
        includes:
            after_body: P3Apendice.Rmd
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

```{r,echo=FALSE,message=FALSE}
library("caret", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("e1071", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("glmnet", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("ROCR", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("leaps", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
```

```{r,echo=FALSE}
set.seed(3)

clasificar_recta = function(punto, recta){
    
    sign(punto[2] - recta[1]*punto[1] - recta[2])
    
}

error_cuadratico = function(clasificados,reales){
    
    squared_error = function(pair_h_y){ # (h(x) - y_n)^2
        (pair_h_y[1]-pair_h_y[2])^2
    }
    
    if(min(reales) == 0){
        
        reales[reales == 0] = -1
        clasificados[clasificados < 0.5] = -1
        clasificados[clasificados >= 0.5] = 1
        
    }
    
    pos_errores = which(clasificados != reales) # Qué etiquetas ha clasificado mal.
    pares_errores = cbind(clasificados[pos_errores],reales[pos_errores],deparse.level=0)
    # Sumamos los errores al cuadrado y hacemos la media.
    sum(apply(pares_errores,1,squared_error))/length(clasificados)
}

porcentaje_error = function(clasificados,reales,fp=1,fn=1){
    
    reales[reales == 0] = -1
    t = table(clasificados,reales)
    total_predicciones = sum(t)
    t[1,2] = t[1,2]*fn
    t[2,1] = t[2,1]*fp
    100*(1-sum(diag(t))/total_predicciones)
    
}

categorizar = function(clasificados,umbral=0.5){
    
    clasificados[clasificados < umbral] = -1
    clasificados[clasificados >= umbral] = 1
    clasificados
    
}

leer_datos_spam = function(){
    
    datos = read.table("./datos/spam.data")
    conjuntos = read.table("./datos/spam.traintest")
    etiquetas = datos[,ncol(datos)]
    list(datos=datos[,-ncol(datos)],etiquetas=etiquetas,conjuntos=conjuntos)
    
}

# Evalúa la regresión para unos datos
evaluar_regresion = function(regresion,datos){
    
    predict(regresion,datos) # Los datos no deben incluir las etiquetas
    
}

# Calcula una cota para E_out en función del error E_test
calcular_cota_eout = function(N,delta) sqrt((log(delta/2))/(-2*N))
```

# Clasificación.

Usaremos la base de datos _SPAM E-mail Database_, que contiene 58 atributos distribuidos de la siguiente forma:

\begin{itemize}

    \item
    48 variables reales continuas en el intervalo $[0,100]$ que corresponden a la proporción en la que aparecen 48 palabras concretas en un correo.
    \item
    6 variables reales continuas en el intervalo $[0,100]$ que corresponden a la proporción en la que aparecen los caracteres ; · ( · [ · ! · \$ · \#.
    \item
    Una variable real continua que representa la longitud media de las secuencias de caracteres en mayúsculas.
    \item
    Una variable entera continua que contiene la longitud de la mayor cadena de caracteres encontrada en mayúsculas.
    \item
    Una variable entera que corresponde al total de mayúsculas encontradas.
    \item
    La variable de respuesta (1 si el correo se consideró spam y 0 si no se consideró spam).

\end{itemize}

Esta base de datos no contiene valores perdidos y su proporción de etiquetas es de $39,4\%$ de positivos y $60,6\%$ de negativos.

Las particiones de entrenamiento y test se encuentran ya definidas de tal forma que usaremos 3065 de los 4601 datos para ajustar los modelos y los 1536 restantes para realizar pruebas.

## Preprocesamiento de datos.

Para realizar el preprocesamiento de datos hemos empleado la función \texttt{preProcess} del paquete \texttt{caret}; esta función permite aplicar distintos métodos de transformación para adecuar las características iniciales a los modelos que vamos a emplear.

A continuación vamos a explicar los métodos utilizados:

\begin{itemize}

    \item
    Transformación de \textit{Yeo-Johnson}: es muy similar a la transformación de \textit{Box-Cox} pero, a diferencia de ésta, permite la existencia de valores negativos ó 0. En nuestro caso, no existen valores negativos pero sí es bastante frecuente encontrar valores iguales a 0.
    \item
    Centrado: se realiza la media de los valores de cada característica y se le resta a cada valor particular, siendo la nueva media igual a 0. Esto reduce las distancias entre las distintas características.
    \item
    Escalado: este método divide los valores por su desviación típica. El objetivo de esta transformación es tener características con rangos uniformes para evitar problemas con modelos que consideran distancias entre valores (lo cual favorecería las características con valores más separados entre sí); además, la independencia respecto a la escala en la que se realizaron las medidas también es importante.
    \item
    Análisis de componentes principales (\textit{PCA}): se buscan nuevos ejes de coordenadas de forma que la varianza de algunas características sea lo suficientemente pequeña como para descartarlas. Como resultado se consigue una reducción de dimensionalidad donde las nuevas características, que no guardan relación de significado con las originales, representarán la parte más significativa de la varianza de los datos.

\end{itemize}

Otro método que se podría haber utilizado para reducir la dimensionalidad de los datos es _Near Zero Variance_, que elimina predictores con varianza cercana a 0, ya que esto suele indicar que su valor es casi constante a lo largo de la muestra y apenas aportan información. En el caso de este conjunto de datos muchas de las características representan frecuencias de aparición de palabras concretas, por lo que es de esperar que en muchas observaciones estos valores sean 0; sin embargo, este hecho no implica que sus contribuciones no tengan importancia.

\newpage

Una vez explicadas las transformaciones, veamos cómo se traduce en la práctica a código:

```{r,cache=TRUE}
# Lectura de datos y obtención del conjunto de entrenamiento
spam = leer_datos_spam()
indices_train = which(spam$conjuntos == 0)

# Función que encapsula el preprocesado de datos
preprocesar_datos = function(datos,indices_train,metodos,umbral_varianza){
    
    preprocess_obj = preProcess(datos[indices_train,],method=metodos,umbral_varianza)
    nuevosDatos = predict(preprocess_obj,datos)
    
}

# Preprocesamiento (Yeo-Johnson, centrado, escalado, análisis de componentes principales...)
spam_procesado = preprocesar_datos(spam$datos,indices_train,
                                   c("YeoJohnson","center","scale","pca"),0.85)
# Sin PCA
spam_procesado_sin_pca = preprocesar_datos(spam$datos,indices_train,
                                           c("YeoJohnson","center","scale"),0.85)
```
```{r,echo=FALSE,cache=TRUE}
# Añadir las etiquetas para la regresión lineal
spam_procesado = cbind(spam_procesado,spam$etiquetas)
spam_procesado_sin_pca = cbind(spam_procesado_sin_pca,spam$etiquetas)
colnames(spam_procesado)[ncol(spam_procesado)] = "etiquetas"
colnames(spam_procesado_sin_pca)[ncol(spam_procesado_sin_pca)] = "etiquetas"
```

Ya que existe la posibilidad de que el análisis de componentes principales elimine características relevantes, vamos a realizar experimentos separados preprocesando los datos con y sin esta técnica.

## Clases de funciones a utilizar.

Ya que esta práctica trata de ajustes de modelos lineales, las clases de funciones a utilizar serán, en principio, las correspondientes a polinomios de grado 1.

No obstante, si observásemos indicios de no linealidad, podríamos plantearnos realizar algún tipo de transformación no lineal utilizando, por ejemplo, los polinomios de _Legendre_.

## Regularización.

Es bastante común enfrentarnos al sobreajuste derivado de considerar una clase demasiado compleja sobre los datos de entrenamiento. Según el principio de la _navaja de Ockham_, la explicación más simple suele ser la correcta, por lo que una posible aproximación puede ser la simplificación.

La regularización toma este concepto para proponer una solución al sobreajuste. La metodología se basa en penalizar los valores altos en las componentes del vector de pesos de la función (ya sea forzando que algunos pesos sean 0 ó restringiendo los valores de todos ellos, entre otras formas).

En el caso de las funciones lineales, la regularización no parece en principio necesaria, debido a que únicamente consideraremos hiperplanos, el tipo más simple de clasificadores. Aun así, comprobaremos los efectos de aplicar esta técnica para asegurarnos que no es necesaria; en concreto, utilizaremos _weight decay_, proporcionado por el paquete \texttt{glmnet}.

\newpage

## Modelos utilizados.

En primer lugar, es necesario hablar de \texttt{regsubsets}, función mediante la cual seleccionaremos los conjuntos de  características con las que construiremos distintos modelos para predecir la variable de respuesta. Dicha función proporciona, para un parámetro $n$, los conjuntos $\{C_1,C_2,\ldots,C_i,\ldots,C_n : |C_i| = i\}$ de los mejores predictores que encuentra. Esta selección de características se puede realizar de varias maneras diferentes:

\begin{itemize}

    \item
    Hacia delante: se comienza sin variables y se añade, tras probarlas todas, la que produce una mejora más significativa del ajuste. Se repite este proceso hasta que ninguna incrementa la efectividad de forma estadísticamente notable.
    \item
    Hacia atrás: se empieza contando con todas las variables y se elimina la que empeora de forma menos significativa la calidad del ajuste. El proceso se repite hasta que ninguna variable se puede eliminar sin deteriorar seriamente el modelo.
    \item
    \textit{Sequential Replacement}: una combinación de variables predictoras (modelo) se representa mediante un vector y está asociada a una medida de su calidad. En cada iteración se realizan todos los reemplazos posibles de variables originales por otras variables y se escoge la mejor nueva combinación, que será el punto de partida de la siguiente iteración. Este proceso se repite hasta que no haya mejora (convergencia).
    \item
    Exhaustiva: se utiliza \textit{Branch and Bound} para realizar una búsqueda lo más completa posible manteniendo un nivel aceptable de eficiencia.
    
\end{itemize}

En los casos en los que se haya realizado una reducción de dimensionalidad importante usaremos una búsqueda exhaustiva. Sin embargo, si tenemos un número considerable de características deberemos optar por métodos más rápidos ya que el tiempo de ejecución la convertiría en inviable; en particular, si no utilizamos análisis de componentes principales, usaremos la búsqueda hacia delante.

La función que utiliza \texttt{regsubsets} para obtener los subconjuntos de variables con los que vamos a probar en las distintas regresiones es la siguiente:

```{r}
subconjuntos_formulas = function(datos,max_tam,metodo="exhaustive"){
    # Obtenemos los subconjuntos de variables
    subsets = regsubsets(etiquetas~.,data=datos,method=metodo,nvmax=max_tam)
    # Obtenemos la matriz de características seleccionadas por grupos de tamaño [1,nvmax]
    matriz_subsets = summary(subsets)$which[,-1]
    # Guardamos, para cada fila, las columnas cuyas variables han sido seleccionadas.
    seleccionados = apply(matriz_subsets,1,which)
    # Obtenemos los nombres de esas columnas (para utilizarlos en la regresión)
    seleccionados = lapply(seleccionados,names)
    # Construimos la suma de las variables que usaremos en la regresión lineal
    seleccionados = mapply(paste,seleccionados,MoreArgs=list(collapse="+"))
    # Construimos strings equivalentes a las fórmulas que usaremos en la regresión lineal
    formulas = mapply(paste,rep("etiquetas~",max_tam),seleccionados,USE.NAMES = FALSE)
    # Construimos objetos fórmula
    formulas = apply(matrix(formulas,nrow=length(formulas)), 1, as.formula)
    list(formulas=formulas,cp=summary(subsets)$cp,bic=summary(subsets)$bic)
}
```

```{r,echo=FALSE,cache=TRUE}
# Buscamos exhaustivamente conjuntos de características que usar
max_caracteristicas = ncol(spam_procesado)-1
max_caracteristicas_sin_pca = ncol(spam_procesado_sin_pca)-1
# Calculamos los objetos fórmula para cada subconjunto de variables
seleccion_caracteristicas = subconjuntos_formulas(spam_procesado[indices_train,],max_caracteristicas)
formulas = seleccion_caracteristicas$formulas
seleccion_caracteristicas_sin_pca = subconjuntos_formulas(spam_procesado_sin_pca[indices_train,],max_caracteristicas_sin_pca,metodo="forward")
formulas_sin_pca = seleccion_caracteristicas_sin_pca$formulas
```

En esta función comenzamos obteniendo una matriz que nos indica qué variables se seleccionan para cada tamaño; a continuación, extraemos los nombres de las características para posteriormente construir sus fórmulas asociadas y así poder utilizarlas como argumento en las etapas siguientes de la experimentación.

Como modelos básicos de ajuste hemos utilizado la regresión lineal con una familia de funciones gaussiana que implementa la función \texttt{lm} y la regresión lineal logística proporcionada como una de las posibilidades de la función \texttt{glm}. Asimismo, hemos considerado el uso de regularización para comprobar empíricamente si es útil en nuestro caso.

Para realizar los experimentos sobre una base común, la misma colección de conjuntos de predictores obtenida mediante \texttt{regsubsets} será empleada tanto con \texttt{lm} como con \texttt{glm}. Para ambas funciones, la decisión de qué conjunto representa el mejor modelo se regirá por distintos criterios: la proporción de etiquetas incorrectas frente al total y los estimadores _Bayesian information criterion_ (BIC) y $C_p$. 

Comenzando por la regresión lineal de la función \texttt{lm}, los únicos parámetros que necesita son los datos de entrenamiento ya procesados, sus etiquetas y las variables que va a utilizar para calcular la regresión. La función que encapsula el proceso hasta el cálculo del porcentaje de error es la siguiente:

```{r}
evalua_lm = function(formula,datos,subconjunto,fp=1,fn=1){
    # do.call pasa los parámetros a lm de forma correcta
    reg_lin = do.call("lm", list(formula=formula, data=substitute(datos),
                                 subset=substitute(subconjunto)))
    prediccion_test = evaluar_regresion(reg_lin,datos[-subconjunto,-ncol(datos)])
    porc_error = porcentaje_error(categorizar(prediccion_test),
                                  datos[-subconjunto,ncol(datos)],fp,fn)
    list(formula=formula, error = porc_error)
}
```

Para utilizar lo anterior en los datos con y sin selección de características sólo necesitamos las siguientes líneas:

```{r,cache=TRUE}
ajustes_lm = mapply(evalua_lm, formulas, MoreArgs = list(datos = spam_procesado,
                                                    subconjunto = indices_train))
ajustes_lm_sin_pca = mapply(evalua_lm, formulas_sin_pca,
                            MoreArgs = list(datos = spam_procesado_sin_pca,
                                            subconjunto = indices_train))
```

Del mismo modo, utilizaremos la función \texttt{glm} para calcular regresiones logísticas. Los parámetros requeridos son los mismos que para \texttt{lm} aunque con la inclusión de la familia de funciones que queremos utilizar, que por defecto es la gaussiana (correspondiente a la regresión lineal). Nosotros emplearemos la binomial, ya que trata con la probabilidad de etiquetar una observación en una de dos posibles categorías; en otras palabras, implementa la regresión logística. De nuevo, la función de \texttt{R} que encapsula el uso de \texttt{glm} se muestra a continuación:

```{r}
evalua_glm = function(formula,datos,subconjunto,fp=1,fn=1,familia=binomial()){
    reg_lin = do.call("glm", list(formula=formula, data=substitute(datos),
                                  subset=substitute(subconjunto),familia))
    prediccion_test = evaluar_regresion(reg_lin,datos[-subconjunto,-ncol(datos)])
    porc_error = porcentaje_error(categorizar(prediccion_test),
                                  datos[-subconjunto,ncol(datos)],fp,fn)
    list(formula=formula, error=porc_error,reg=reg_lin)
}
```

Las llamadas a función para calcular las regresiones logísticas quedan, pues, de la siguiente forma:

```{r,cache=TRUE}
ajustes_glm = mapply(evalua_glm, formulas, 
                     MoreArgs = list(datos = spam_procesado, 
                                     subconjunto = indices_train))
ajustes_glm_sin_pca = mapply(evalua_glm, formulas_sin_pca, 
                             MoreArgs = list(datos = spam_procesado_sin_pca,
                                             subconjunto = indices_train))
```

Finalmente, antes de aplicar regularización debemos estudiar en qué circunstancias tiene sentido. Una vez realizado el análisis de componentes principales, el cual ya elimina presumiblemente características que conduzcan a un sobreajuste, la regularización podría provocar _underfitting_ o infraajuste. No obstante, si omitimos la etapa de PCA este problema tiene menos posibilidades de presentarse, por lo que realizaremos el experimento en este último caso.

\newpage

El paquete \texttt{glmnet} implementa la funcionalidad que necesitamos. La regularización precisa de un hiperparámetro $\lambda$ que hemos de calcular previamente usando la función \texttt{cv.glmnet}, la cual aplica validación cruzada para obtener el mejor valor de $\lambda$ (el que obtiene una menor tasa de error).

```{r,cache=TRUE}
# Creamos la matriz de datos en el formato que necesita glmnet
x = model.matrix(etiquetas~.,spam_procesado_sin_pca)[,-ncol(spam_procesado_sin_pca)]
y = spam_procesado_sin_pca$etiquetas
# Obtenemos los errores de validación cruzada en el conjunto
cv.out = cv.glmnet(x[indices_train,],y[indices_train],alpha=0)
# Guardamos el lambda que ha dado menor error de validación cruzada
bestlambda = cv.out$lambda.min
```

La función \texttt{cv.glmnet} devuelve, entre otros datos, un modelo que no consideraremos ya que es resultado del proceso de validación cruzada; esto significa que ha sido ajustado con un subconjunto del conjunto de entrenamiento y posiblemente no tenga la calidad suficiente.

Por ello, vamos a hacer uso de la función \texttt{glmnet}; ésta recibe como parámetros los datos y etiquetas en el mismo formato que \texttt{cv.glmnet}, además de un $\alpha$ que varía entre 0 y 1 e indica la proporción en la que se aplican respectivamente _weight decay_ y _LASSO_, y un conjunto de valores de $\lambda$ que se encargará de probar (en nuestro caso, sólo el mejor $\lambda$ obtenido como ya hemos visto).

```{r, cache=TRUE}
# Obtenemos un modelo de Ridge
modelo_ridge = glmnet(x,y,alpha=0,lambda=bestlambda)
# Calculamos las predicciones y el error asociado a ellas
modelo_ridge.pred = predict(modelo_ridge,s=bestlambda,newx=x[-indices_train,])
error_ridge = porcentaje_error(categorizar(modelo_ridge.pred),
                               spam_procesado[-indices_train,ncol(spam_procesado)],fp=1)
```

Una vez obtenidos los modelos, es el momento de estudiar sus tasas de error y elegir el mejor de ellos; lo haremos, pues, en la siguiente sección.

## Selección del modelo final.

Para seleccionar un modelo estudiaremos en primer lugar las tasas de error. El cálculo de dicho error hace uso de la matriz de confusión y se ha implementado como se muestra a continuación, donde \texttt{fp} y \texttt{fn} corresponden a las penalizaciones que se le dan a falsos positivos y falsos negativos respectivamente. En este caso ambos tendrán el mismo coste ya que, aunque la semántica de nuestro problema sugiere que los falsos positivos se penalicen sobre los falsos negativos, no se proporciona una matriz de coste.

```{r}
porcentaje_error = function(clasificados,reales,fp=1,fn=1){
    reales[reales == 0] = -1
    t = table(clasificados,reales)
    total_predicciones = sum(t)
    t[1,2] = t[1,2]*fn
    t[2,1] = t[2,1]*fp
    100*(1-sum(diag(t))/total_predicciones)
}
```

\newpage

Como hemos generado varios conjuntos de modelos utilizando las funciones \texttt{lm} y \texttt{glm}, podemos comenzar comparando y eligiendo un modelo que represente cada uno de estos conjuntos (el mejor de ellos). Las siguientes gráficas muestran los errores obtenidos con diferentes subconjuntos de predictores para los datos con y sin análisis de componentes principales:

\vspace{1cm}
```{r, echo=FALSE}
par(mfrow=c(2,2))
# Con PCA

lm_min_error_index = which.min(unlist(ajustes_lm[2,]))
glm_min_error_index = which.min(unlist(ajustes_glm[2,]))
min_bic_index = which.min(seleccion_caracteristicas$bic)
min_cp_index = which.min(seleccion_caracteristicas$cp)

plot(x=1:ncol(ajustes_lm),y=ajustes_lm[2,],pch=20,ylim=c(6,13),col="blue",xlab="Tamaño del conjunto",ylab="Error porcentual", main = "Regresión lineal con PCA")
lines(x=1:ncol(ajustes_lm),y=ajustes_lm[2,],pch=20,col="blue")
points(x=lm_min_error_index, y=ajustes_lm[2,lm_min_error_index], pch=10, col="red")
points(x=min_cp_index, y=ajustes_lm[2,min_cp_index], pch=10, col="green")
points(x=min_bic_index, y=ajustes_lm[2,min_bic_index], pch=10, col="orange")

plot(x=1:ncol(ajustes_glm),y=ajustes_glm[2,],pch=20,ylim=c(6,13),col="blue",xlab="Tamaño del conjunto",ylab="Error porcentual", main = "Regresión logística con PCA")
lines(x=1:ncol(ajustes_glm),y=ajustes_glm[2,],pch=20,col="blue")
points(x=glm_min_error_index, y=ajustes_glm[2,glm_min_error_index], pch=10, col="red")
points(x=min_cp_index, y=ajustes_glm[2,min_cp_index], pch=10, col="green")
points(x=min_bic_index, y=ajustes_glm[2,min_bic_index], pch=10, col="orange")

# Sin PCA

lm_sp_min_error_index = which.min(unlist(ajustes_lm_sin_pca[2,]))
glm_sp_min_error_index = which.min(unlist(ajustes_glm_sin_pca[2,]))
sp_min_cp_index = which.min(seleccion_caracteristicas_sin_pca$cp)
sp_min_bic_index = which.min(seleccion_caracteristicas_sin_pca$bic)

plot(x=1:ncol(ajustes_lm_sin_pca),y=ajustes_lm_sin_pca[2,],pch=20,ylim=c(6,21),col="blue",xlab="Tamaño del conjunto",ylab="Error porcentual", main = "Regresión lineal sin PCA")
lines(x=1:ncol(ajustes_lm_sin_pca),y=ajustes_lm_sin_pca[2,],pch=20,col="blue")
points(x=lm_sp_min_error_index, y=ajustes_lm_sin_pca[2,lm_sp_min_error_index], pch=10, col="red")
points(x=sp_min_cp_index, y=ajustes_lm_sin_pca[2,sp_min_cp_index], pch=10, col="green")
points(x=sp_min_bic_index, y=ajustes_lm_sin_pca[2,sp_min_bic_index], pch=10, col="orange")

plot(x=1:ncol(ajustes_glm_sin_pca),y=ajustes_glm_sin_pca[2,],pch=20,ylim=c(6,21),col="blue",xlab="Tamaño del conjunto",ylab="Error porcentual", main = "Regresión logística sin PCA")
lines(x=1:ncol(ajustes_glm_sin_pca),y=ajustes_glm_sin_pca[2,],pch=20,col="blue")
points(x=glm_sp_min_error_index, y=ajustes_glm_sin_pca[2,glm_sp_min_error_index], pch=10, col="red")
points(x=sp_min_cp_index, y=ajustes_glm_sin_pca[2,sp_min_cp_index], pch=10, col="green")
points(x=sp_min_bic_index, y=ajustes_glm_sin_pca[2,sp_min_bic_index], pch=10, col="orange")
par(c(1,1), pty = "m")
```
\vspace{1cm}

Los modelos con los que se obtiene el mínimo $E_{test}$ se han marcado en rojo; adicionalmente, se han destacado en verde los obtenidos con $C_p$ y en naranja los obtenidos con _BIC_, ambos proporcionados por \texttt{regsubsets}. El estimador $C_p$, además de utilizar el error cuadrático estimado del ajuste realizado por un subconjunto de predictores, añade una penalización al número de características escogidas. Su objetivo es evitar el sobreajuste, puesto que un número de predictores elevado puede resultar en que características que no son de relevancia jueguen un papel importante en el modelo, obteniendo resultados pobres al aplicarse a nuevas observaciones. _BIC_ es un estimador similar que se diferencia del primero en el cálculo de la penalización. Para ello, tiene en cuenta el número total de observaciones además del número de características seleccionadas, por lo que generalmente elige modelos con menos predictores que $C_p$. 

En la siguiente tabla se muestran, para los distintos tipos de ajuste y datos, los modelos seleccionados siguiendo los tres criterios descritos. Para cada modelo se proporcionan el $E_{test}$ y el número de predictores utilizado.

|                     |   Error Mínimo   |          | Estimador $C_p$     |          | Estimador  _BIC_     |          |
|---------------------|:----------------:|:--------:|:-------------------:|:--------:|:--------------------:|:--------:|
|                     | $E_{test}$       | Tamaño   | $E_{test}$          | Tamaño   | $E_{test}$           | Tamaño   |
| Ajustes lm          | 7.226562         | 33       | 7.291667            | 31       | 7.942708             | 22       |
| Ajustes glm         | 6.184896         | 23       | 6.445312            | 31       | 6.510417             | 22       |
| Ajustes lm sin PCA  | 6.705729         | 28       | 6.705729            | 39       | 6.966146             | 26       |
| Ajustes glm sin PCA | 5.989583         | 56       | 6.315104            | 34       | 6.510417             | 26       |

En la tabla podemos observar una diferencia de calidad entre el modelo de regresión logística sin PCA de 56 características y los demás. Esto podría conducirnos a elegirlo sin más como nuestro modelo definitivo, ya que en general parece razonable escoger el modelo que da el menor error en la partición de test. Sin embargo, en algunos casos podemos preferir un modelo con un error ligeramente superior a cambio de una reducción sustancial de dimensionalidad. Por ello, vamos a representar las cuatro gráficas anteriores conjuntamente para ver mejor si existe una alternativa de este tipo:

\vspace{1cm}
```{r,echo=FALSE}
plot(x=10:ncol(ajustes_lm),y=ajustes_lm[2,-(1:9)],type="o",pch=20,xlim=c(10,57),ylim=c(5,10),col="blue",xlab="Tamaño del conjunto",ylab="Error porcentual", main = "Comparativa")
points(x=10:ncol(ajustes_glm),y=ajustes_glm[2,-(1:9)],type="o",pch=20,col="red")
points(x=10:ncol(ajustes_lm_sin_pca),y=ajustes_lm_sin_pca[2,-(1:9)],type="o",pch=20,col="green")
points(x=10:ncol(ajustes_glm_sin_pca),y=ajustes_glm_sin_pca[2,-(1:9)],type="o",pch=20,col="orange")
legend(36.5,10,c("R. Lineal","R. Logística", "R. Lineal sin PCA", "R. Logística sin PCA"),lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red","green","orange"))
```
\vspace{1cm}

Visualmente podemos deducir lo que un análisis más detallado de los números nos diría: asumiendo un pequeño aumento de $E_{test}$ (6.12 frente a 5.98), podríamos quedarnos con un modelo de regresión logística sin PCA con hasta 16 características menos; otras alternativas serían un $E_{test}$ de 6.05 con 2 características menos o un $E_{test}$ de 6.18 con 33 características menos con regresión logística usando PCA.

Adicionalmente, antes de considerar cuál de los cuatro modelos anteriores vamos a escoger, sería interesante recordar el ajuste con regularización que hemos mencionado en la sección anterior para saber si nos puede aportar algo nuevo. El porcentaje de error que presenta es de un 6.57 \%, insuficiente en comparación con los otros modelos. Esto es debido a que, como ya habíamos adelantado, la poca complejidad de nuestras funciones reduce la utilidad de esta técnica; por ello, tomamos la decisión de descartar la regularización.

Por último, vamos a realizar sobre los cuatro modelos otro tipo de análisis, el basado en la curva _ROC_ (_Receiver Operating Characteristic_); esta curva representa la tasa de verdaderos positivos frente a la tasa de falsos positivos, y el área bajo ella nos dará una medida de lo acertado de las predicciones de un modelo.

En \texttt{R} podemos trabajar con curvas _ROC_ usando el paquete \texttt{ROCR}. Vamos a ver una función que a partir de unas predicciones y unas etiquetas reales nos devuelve el valor de área y la curva lista para ser representada con \texttt{plot}:

```{r}
calcula_curva_roc = function(pred,truth){
    predob = prediction(pred,truth)
    area = performance(predob,"auc")
    curva = performance(predob,"tpr","fpr")
    list(curva=curva,area=area)
}
```

Las cuatro curvas _ROC_ generadas quedan de la siguiente manera:

\vspace{1cm}
```{r,echo=FALSE}
roc_glm_min_err = calcula_curva_roc(evaluar_regresion(ajustes_glm[3,glm_min_error_index],spam_procesado[-indices_train,-ncol(spam_procesado)]),spam_procesado[-indices_train,ncol(spam_procesado)])
roc_glm_sin_pca_min_err = calcula_curva_roc(evaluar_regresion(ajustes_glm_sin_pca[3,glm_min_error_index],spam_procesado_sin_pca[-indices_train,-ncol(spam_procesado_sin_pca)]),spam_procesado_sin_pca[-indices_train,ncol(spam_procesado_sin_pca)])
roc_glm_sin_pca_40 = calcula_curva_roc(evaluar_regresion(ajustes_glm_sin_pca[3,40],spam_procesado_sin_pca[-indices_train,-ncol(spam_procesado_sin_pca)]),spam_procesado_sin_pca[-indices_train,ncol(spam_procesado_sin_pca)])
roc_glm_sin_pca_54 = calcula_curva_roc(evaluar_regresion(ajustes_glm_sin_pca[3,54],spam_procesado_sin_pca[-indices_train,-ncol(spam_procesado_sin_pca)]),spam_procesado_sin_pca[-indices_train,ncol(spam_procesado_sin_pca)])

par(mfrow=c(2,2))
par(pty="s")
plot(roc_glm_min_err$curva,main="R. Logística, 23 variables, PCA",col="blue",lwd=1.5)
plot(roc_glm_sin_pca_min_err$curva,main="R. Logística, 56 variables, sin PCA",col="blue",lwd=1.5)
plot(roc_glm_sin_pca_40$curva,main="R. Logística, 40 variables, sin PCA",col="blue",lwd=1.5)
plot(roc_glm_sin_pca_54$curva,main="R. Logística, 54 variables, sin PCA",col="blue",lwd=1.5)
par(pty="m")
par(mfrow=c(1,1))
```

En las gráficas anteriores se puede observar una gran similitud, por otra parte esperada, entre los cuatro modelos. Ya que no podemos distinguir de esta forma cuál es el mejor según este criterio, vamos a obtener el área bajo las curvas (_AUC_) para trabajar con una medida objetiva.

|     | 23 variables, PCA | 40 variables sin PCA | 54 variables sin PCA | 56 variables sin PCA |
|:---:|:-----------------:|:--------------------:|:--------------------:|:--------------------:|
| AUC |     0.9458017     |       0.9807312      |       0.9819779      |       0.9771073      |

Basándonos tanto en las tasas de error $E_{test}$ como en las áreas bajo las curvas _ROC_, hemos decidido que el modelo de regresión logística sin análisis de componentes principales utilizando 40 predictores presenta el balance más razonable entre dimensionalidad y precisión.