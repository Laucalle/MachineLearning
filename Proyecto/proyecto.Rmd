---
lang: es
date: "1 de junio de 2017"
output: 
    pdf_document:
        fig_caption: yes
        toc: true
        number_sections: yes
        highlight: pygments
        includes:
            in_header: header.tex
            before_body: titlepage.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

```{r,echo=FALSE,message=FALSE}
library("neuralnet", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("caret", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("e1071", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("glmnet", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("ROCR", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("leaps", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("randomForest", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("ada", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
library("kernlab", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
```

```{r,echo=FALSE}
set.seed(3)

leer_datos_partidos = function(){

    datos = read.csv("./datos/partidos.csv", header = TRUE)
    datos = datos[,-c(1,2,3)]
    # Añadimos tipo de pista como característica
    pista_dura = c(rep(1,253),rep(0,252),rep(1,202),rep(0,236))
    tierra_batida = c(rep(0,253),rep(1,252),rep(0,438))
    datos = cbind(datos,pista_dura,tierra_batida)
    etiquetas = datos[,1]
    indices = sample(nrow(datos),round(0.7*nrow(datos)))
    list(datos=datos[,-1],etiquetas=etiquetas,indices_train=indices)
    
}

# Preprocesamiento (centrado, escalado, análisis de componentes principales...)
preprocesar_datos = function(datos,indices_train,metodos,umbral_varianza=0.9){
    
    preprocess_obj = preProcess(datos[indices_train,],method=metodos,umbral_varianza)
    nuevosDatos = predict(preprocess_obj,datos)
    
}

# Evalúa la regresión para unos datos
evaluar_modelo = function(regresion,datos){
    
    predict(regresion,datos) # Los datos no deben incluir las etiquetas
    
}

porcentaje_error = function(clasificados,reales,fp=1,fn=1){
    
    reales[reales == 0] = -1
    t = table(clasificados,reales)
    total_predicciones = sum(t)
    t[1,2] = t[1,2]*fn
    t[2,1] = t[2,1]*fp
    100*(1-sum(diag(t))/total_predicciones)
    
}

categorizar = function(clasificados,umbral=0.5){
    
    clasificados[clasificados < umbral] = -1
    clasificados[clasificados >= umbral] = 1
    clasificados
    
}

# Evalúa una regresión lineal dada una fórmula y unos datos de entrenamiento 
evalua_lm = function(formula,datos,subconjunto,fp=1,fn=1){
    reg_lin = do.call("lm", list(formula=formula, data=substitute(datos), subset=substitute(subconjunto)))
    prediccion_test = evaluar_modelo(reg_lin,datos[-subconjunto,-ncol(datos)])
    porc_error = porcentaje_error(categorizar(prediccion_test),datos[-subconjunto,ncol(datos)],fp,fn)
    list(formula=formula, error = porc_error)
}

evalua_glm = function(formula,datos,subconjunto,fp=1,fn=1,familia=binomial()){
    reg_lin = do.call("glm", list(formula=formula, data=substitute(datos), subset=substitute(subconjunto),familia))
    prediccion_test = evaluar_modelo(reg_lin,datos[-subconjunto,-ncol(datos)])
    porc_error = porcentaje_error(categorizar(prediccion_test),datos[-subconjunto,ncol(datos)],fp,fn)
    list(formula=formula, error=porc_error,reg=reg_lin)
}

calcular_cota_eout = function(N,delta) sqrt((log(delta/2))/(-2*N))
```

# Introducción

Para la realización de este trabajo se ha empleado la base de datos _Tennis Major Tournament Match Statistics_, que recoge información acerca de los partidos disputados en los _Grand Slam_ de 2013 (excepto por el Abierto de Australia, que por alguna razón es el de 2014). Los campos aportados por cada partido son los siguientes:

\begin{itemize}

    \item
    Nombres de los participantes.
    \item
    Resultado (desde el punto de vista del primer participante).
    \item
    Ronda.
    \item
    Porcentajes relativos al primer servicio de cada jugador: jugados y ganados.
    \item
    Porcentajes relativos al segundo servicio de cada jugador: jugados y ganados.
    \item
    Número de \textit{aces} por cada jugador.
    \item
    Número de dobles faltas cometidas por cada jugador.
    \item
    Número de puntos ganadores por cada jugador.
    \item
    Número de errores no forzados cometidos por cada jugador.
    \item
    Número de puntos de \textit{break} creados y ganados por cada jugador.
    \item
    Número de puntos en la red intentados y ganados por cada jugador.
    \item
    Puntos totales ganados por cada jugador.
    \item
    Juegos por set de cada jugador.
    \item
    Número total de sets ganados por cada jugador.
    
\end{itemize}

Esta base de datos contiene valores perdidos. No obstante, todos ellos representan cantidades que se pueden sustituir por cero de forma segura ya que denotan, por ejemplo, ausencia de errores no forzados, ausencia de _aces_ o sets no jugados (por abandono o por no ser necesarios).

Debido a que este conjunto de datos no tiene una variable de respuesta definida explícitamente, vamos a elegir una: el resultado del partido. Para que la predicción no sea trivial, es necesario eliminar algunas características que permiten determinar unívocamente el resultado, como el número de sets ganados por cada jugador y la puntuación dentro de los mismos. Además, por la naturaleza del dominio del problema, aprovecharemos que se nos proporciona implícitamente el tipo de pista en el que se han jugado los partidos para considerarlo como una característica más; ya que es un factor con tres niveles, se añadirán dos variables _dummy_ para contemplarlo (la tercera clase es una combinación lineal de las otras dos). Una vez realizado todo esto, pasaremos de 40 variables a 28 y comprobaremos así si es factible utilizar este nuevo conjunto de predictores con el mismo propósito.

Un aspecto a tener en cuenta es el sesgo del propio conjunto de datos, provocado por la forma de construir las observaciones. Al ser la asignación de *Jugador 1* y *Jugador 2* arbitraria, de forma implícita se está favoreciendo que el mismo estadístico para un jugador tenga más importancia que para otro. Nosotros, por conocimiento general de este problema, sabemos que no importa en qué orden los nombremos; sin embargo, los modelos no tienen esta información y pueden llegar a conclusiones sesgadas basadas en este orden. Por ejemplo, para un mismo partido, la observación en la que el primer jugador comete 10 dobles faltas y el segundo comete 5 es equivalente a la observación en la que el primer jugador comete 5 dobles faltas y el segundo 10, ya que los jugadores son intercambiables. Un modelo no tiene esta información intuitiva y, por tanto, podría dar importancias distintas a variables en teoría simétricas.

En cuanto a la división en conjuntos de entrenamiento y test, no se nos proporciona un criterio de antemano, por lo que en principio la realizaremos nosotros tomando observaciones aleatorias según la proporción 70\%-30\%.

\newpage

# Preprocesamiento de datos.

Para realizar el preprocesamiento de datos hemos empleado la función \texttt{preProcess} del paquete \texttt{caret}; esta función permite aplicar distintos métodos de transformación para adecuar las características iniciales a los modelos que vamos a emplear.

A continuación vamos a explicar los métodos utilizados:

\begin{itemize}

    \item
    Transformación de \textit{Yeo-Johnson}: es muy similar a la transformación de \textit{Box-Cox} pero, a diferencia de ésta, permite la existencia de valores negativos ó 0. En nuestro caso, no existen valores negativos pero sí es bastante frecuente encontrar valores iguales a 0.
    \item
    Centrado: se realiza la media de los valores de cada característica y se le resta a cada valor particular, siendo la nueva media igual a 0. Esto reduce las distancias entre las distintas características.
    \item
    Escalado: este método divide los valores por su desviación típica. El objetivo de esta transformación es tener características con rangos uniformes para evitar problemas con modelos que consideran distancias entre valores (lo cual favorecería las características con valores más separados entre sí); además, la independencia respecto a la escala en la que se realizaron las medidas también es importante.
    \item
    Análisis de componentes principales (\textit{PCA}): se buscan nuevos ejes de coordenadas de forma que la varianza de algunas características sea lo suficientemente pequeña como para descartarlas. Como resultado se consigue una reducción de dimensionalidad donde las nuevas características, que no guardan relación de significado con las originales, representarán la parte más significativa de la varianza de los datos.

\end{itemize}

Otro método que se podría haber utilizado para reducir la dimensionalidad de los datos es _Near Zero Variance_, que elimina predictores con varianza cercana a 0, ya que esto suele indicar que su valor es casi constante a lo largo de la muestra y apenas aportan información. Sin embargo, hemos comprobado empíricamente que no afecta a nuestro conjunto de datos en particular.

Una vez explicadas las transformaciones, veamos cómo se traduce en la práctica a código:

```{r}
# Lectura de datos y obtención del conjunto de entrenamiento
datos = leer_datos_partidos()
etiquetas = datos$etiquetas
indices_train = datos$indices_train
datos = datos$datos

preprocesar_datos = function(datos,indices_train,metodos,umbral_varianza=0.9){
    preprocess_obj = preProcess(datos[indices_train,],method=metodos,umbral_varianza)
    nuevosDatos = predict(preprocess_obj,datos)
}

datos[is.na(datos)] = 0 # Sustituimos los valores perdidos por 0
datos = subset(datos,select=-c(FNL1,FNL2)) # Número de sets ganados por cada uno
datos = subset(datos,select=-c(ST1.1,ST2.1,ST3.1,ST4.1,ST5.1,ST1.2,ST2.2,ST3.2,ST4.2,ST5.2))
datos_procesados = preprocesar_datos(datos,indices_train,
                                     c("YeoJohnson","center","scale","pca"),0.85)
datos_procesados_sin_pca = preprocesar_datos(datos,indices_train,
                                             c("YeoJohnson","center","scale"))
```
```{r,echo=FALSE}
# Añadir las etiquetas para la regresión lineal
datos_procesados = cbind(datos_procesados,etiquetas)
datos_procesados_sin_pca = cbind(datos_procesados_sin_pca,etiquetas)
colnames(datos_procesados)[ncol(datos_procesados)] = "etiquetas"
colnames(datos_procesados_sin_pca)[ncol(datos_procesados_sin_pca)] = "etiquetas"
```

El análisis de componentes principales reduce a 9 los predictores a utilizar. Ya que existe la posibilidad de que este análisis elimine características relevantes, vamos a realizar experimentos separados preprocesando los datos con y sin esta técnica.

# Modelos a estudiar

En principio, podemos comenzar comprobando la calidad de un modelo lineal; tras ello, fuese o no necesario (debido a la naturaleza de este proyecto), probaremos modelos no lineales como _Random Forest_, _Boosting_, _Support Vector Machines_ o redes neuronales.

## Modelos lineales

Ya que el objetivo de este proyecto es analizar la eficacia de modelos no lineales, elegiremos un único modelo lineal de forma no tan exhaustiva como en el trabajo 3.

En primer lugar, vamos a formarnos una idea de la utilidad en este contexto de ambos conjuntos de datos preprocesados guiándonos por la tasa de error que produce una regresión logística con todas sus características:

```{r}
reg_log = evalua_glm(etiquetas~.,datos_procesados,indices_train)
reg_log_sin_pca = evalua_glm(etiquetas~.,datos_procesados_sin_pca,indices_train)
```

El primer ajuste da un error de 8.83\%, frente al segundo, que da un 5.3\%. Según este criterio, elegiremos el segundo modelo; no obstante, ya que utiliza todas las características a su disposición, vamos a tratar de encontrar un subconjunto de ellas con la función \texttt{regsubsets} que mantenga la calidad a la vez que ofrece una reducción de dimensionalidad.

Dicha función es empleada de la siguiente forma para obtener los subconjuntos de variables con los que vamos a calcular las distintas regresiones:

```{r}
subconjuntos_formulas = function(datos,max_tam,metodo="exhaustive"){
    # Obtenemos los subconjuntos de variables
    subsets = regsubsets(etiquetas~.,data=datos,method=metodo,nvmax=max_tam)
    # Obtenemos la matriz de características seleccionadas por grupos de tamaño [1,nvmax]
    matriz_subsets = summary(subsets)$which[,-1]
    # Guardamos, para cada fila, las columnas cuyas variables han sido seleccionadas.
    seleccionados = apply(matriz_subsets,1,which)
    # Obtenemos los nombres de esas columnas (para utilizarlos en la regresión)
    seleccionados = lapply(seleccionados,names)
    # Construimos la suma de las variables que usaremos en la regresión lineal
    seleccionados = mapply(paste,seleccionados,MoreArgs=list(collapse="+"))
    # Construimos strings equivalentes a las fórmulas que usaremos en la regresión lineal
    formulas = mapply(paste,rep("etiquetas~",max_tam),seleccionados,USE.NAMES = FALSE)
    # Construimos objetos fórmula
    formulas = apply(matrix(formulas,nrow=length(formulas)), 1, as.formula)
    list(formulas=formulas,cp=summary(subsets)$cp,bic=summary(subsets)$bic)
}
```

Un ejemplo de uso es:

```{r}
# Seleccionamos subconjuntos de características para los datos con PCA
max_caracteristicas = ncol(datos_procesados)-1
seleccion_caracteristicas = subconjuntos_formulas(datos_procesados[indices_train,],
                                                  max_caracteristicas,metodo="exhaustive")
formulas = seleccion_caracteristicas$formulas
```

```{r,echo=FALSE}
# Seleccionamos subconjuntos de características para los datos sin PCA
max_caracteristicas_sin_pca = ncol(datos_procesados_sin_pca)-1
seleccion_caracteristicas_sin_pca = subconjuntos_formulas(datos_procesados_sin_pca[indices_train,],max_caracteristicas_sin_pca,metodo="exhaustive")
formulas_sin_pca = seleccion_caracteristicas_sin_pca$formulas
```

\newpage

Ahora realizaremos ajustes para todos los subconjuntos de características obtenidos mediante el código anterior:

```{r}
ajustes_glm = mapply(evalua_glm, formulas, 
                     MoreArgs = list(datos = datos_procesados, 
                                     subconjunto = indices_train))
ajustes_glm_sin_pca = mapply(evalua_glm, formulas_sin_pca, 
                             MoreArgs = list(datos = datos_procesados_sin_pca, 
                                             subconjunto = indices_train))
```

Para comparar más fácilmente los porcentajes de error obtenidos por los modelos, vamos a representarlos en la siguiente gráfica:

\vspace{0.7cm}

```{r,echo=FALSE,,fig.align='center'}
glm_min_error_index = which.min(unlist(ajustes_glm[2,]))
glm_sin_pca_min_error_index = which.min(unlist(ajustes_glm_sin_pca[2,]))
plot(x=1:ncol(ajustes_glm_sin_pca),y=ajustes_glm_sin_pca[2,],pch=20,ylim=c(4,30),type="o",col="blue",xlab="Tamaño del conjunto",ylab="Error porcentual", main = "Comparativa de regresiones logísticas")
points(x=glm_sin_pca_min_error_index, y=ajustes_glm_sin_pca[2,glm_sin_pca_min_error_index], pch=19, col="orange")
points(x=1:ncol(ajustes_glm),y=ajustes_glm[2,],type="o",pch=20,col="red")
points(x=glm_min_error_index, y=ajustes_glm[2,glm_min_error_index],pch=19,col="green")
legend(16.5,29,c("Sin PCA","Con PCA"),lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
```

\vspace{0.7cm}

El punto señalado en verde nos indica el conjunto de predictores cuyo error ha sido menor (8.48\%) para los datos con análisis de componentes principales. Análogamente, el punto naranja señala el conjunto de características que ha dado menor error (4.59\%) para los datos sin PCA.

Ya que hemos conseguido no sólo reducir el número de predictores necesarios (de 28 a 13) sino también el error, vamos a elegir el modelo que proporciona un 4.59\% de error para futuras comparaciones con modelos no lineales.

\newpage

## Modelos no lineales

Procediendo de forma similar, analizaremos varios modelos no lineales utilizando los dos conjuntos de características obtenidos en la sección de preprocesamiento de datos. Antes de empezar, es preciso apuntar que los ajustes lineales anteriores han obtenido errores bastante bajos, por lo que la complejidad adicional introducida por las técnicas que veremos a continuación deberá tener asociado un error aún más bajo que la justifique.

Para automatizar la optimización de hiperparámetros mediante validación cruzada utilizaremos las funcionalidades proporcionadas por el paquete \texttt{caret}; en concreto, la función \texttt{train}. Esto se explorará con mayor profundidad en los apartados dedicados a cada tipo de modelo.

### _Random Forest_

```{r,echo=FALSE}
evalua_random_forest_cv = function(datos,etiquetas,arboles=100,k=10,i=1,metodo="cv"){
    mtry = sqrt(ncol(datos))
    rf_train_control = trainControl(method=metodo,number=k,repeats=i)
    rf_train = train(datos,as.factor(etiquetas),method="rf",ntree=arboles,preProcess=c("YeoJohnson","center","scale"),trControl=rf_train_control,tuneGrid=expand.grid(.mtry=mtry))
    rf_error = (1-rf_train$results$Accuracy)*100
    list(arboles=arboles,error=rf_error,rf=rf_train)
}
set.seed(111)
```

El primer modelo no lineal que vamos a estudiar será _Random Forest_. Esta técnica consiste en
entrenar un cierto número de árboles usando subconjuntos aleatorios de características para después promediar sus resultados.

Partiendo del concepto de _bagging_, _Random Forest_ muestrea con reemplazo el conjunto de $n$ datos original para obtener $p$ conjuntos de un cierto tamaño $n'$ con los que ajustar $p$ árboles. Lo que diferencia estas dos técnicas es la elección de subconjuntos aleatorios de variables utilizadas en el ajuste de cada árbol que realiza _Random Forest_.

Como hemos comentado, vamos a utilizar \texttt{train} para encontrar, en este caso, el número de árboles (\texttt{ntree}) más adecuado para una cantidad de variables por árbol fija.

Para empezar, vamos a especificar que queremos realizar validación cruzada. Esto se hace creando un objeto \texttt{trainControl} que posteriormente \texttt{train} tomará como argumento.

```{r}
control = trainControl(method = "cv", number = 10) # 10-fold cv
```

El siguiente paso es crear un conjunto de posibles valores de \texttt{ntree}:

```{r}
num_arboles = seq(50,500,5)
```

Utilizaremos la función que se muestra a continuación para obtener el error de validación cruzada para cada uno de esos valores:

```{r ajustes_rf_cv, fig.align='center', warning=FALSE, cache=TRUE}
evalua_random_forest_cv = function(datos,etiquetas,control,arboles=100){
    mtry = sqrt(ncol(datos))
    rf_train = train(datos,as.factor(etiquetas),
                     method="rf",ntree=arboles,
                     preProcess=c("YeoJohnson","center","scale"),
                     trControl=control,tuneGrid=expand.grid(.mtry=mtry))
    rf_error = (1-rf_train$results$Accuracy)*100
    list(arboles=arboles,error=rf_error,rf=rf_train)
}

ajustes_rf_cv = mapply(evalua_random_forest_cv,num_arboles,
                       MoreArgs = list(datos=datos[indices_train,],
                                       etiquetas=etiquetas[indices_train],
                                       control=control))
```

Siguiendo las directrices del proyecto, mantendremos constante el número de variables predictoras a $\sqrt{n}$, donde $n$ es el total de características disponibles.

Los parámetros principales que recibe \texttt{train} son, por este orden: los datos originales, las etiquetas, el método (en nuestro caso, _Random Forest_), el número de árboles específico y las transformaciones de preprocesamiento. Además, hemos de pasarle el objeto \texttt{control} y el número de variables definidos previamente. Finalmente, debemos tener en cuenta otros parámetros que no aparecen en el código por tener valores correctos por defecto: \texttt{replace} (muestreo con o sin reemplazo) o \texttt{sampsize} ($n'$, tamaño de los conjuntos obtenidos).

Ya que vamos a realizar validación cruzada, es importante no contaminar el conjunto de datos realizando el preprocesamiento antes de crear las particiones de validación. La razón de esto es que esta técnica toma información de todos los datos a su disposición para homogeneizar los valores de las características, por lo que si preprocesamos el conjunto completo las particiones de validación no serán totalmente independientes. Es por esto que \texttt{train} aplica las transformaciones en cada iteración de la validación cruzada.

Una vez obtenidos los $E_{CV}$, decidiremos qué número de árboles es el más adecuado entre los candidatos según este criterio. Esta decisión se ilustra mejor con la siguiente gráfica:

\vspace{0.7cm}
```{r,echo=FALSE}
rf_cv_min_error_index = which.min(unlist(ajustes_rf_cv[2,]))

plot(x=num_arboles,y=ajustes_rf_cv[2,],pch=20,ylim=c(4,19),type="o",col="blue",xlab="Número de árboles",ylab="% Error de validación cruzada", main = "Comparativa de número de árboles")
points(x=ajustes_rf_cv[1,rf_cv_min_error_index], y=ajustes_rf_cv[2,rf_cv_min_error_index], pch=19, col="orange")
```
\vspace{0.7cm}

El punto señalado en naranja indica el número de árboles con el que se ha obtenido el menor $E_{CV}$: 465. Por ello, ajustaremos un modelo con 465 árboles utilizando toda la partición de entrenamiento para posteriormente evaluar su calidad sobre la partición de test.

\newpage

Para finalizar, veamos cómo ha evolucionado el error de este modelo conforme aumenta su número de árboles:

\vspace{0.7cm}
```{r,echo=FALSE,fig.align='center'}
plot(ajustes_rf_cv[3,rf_cv_min_error_index]$rf$finalModel,lty=1,main="Evolución del mejor Random Forest")
legend(290,0.25,c("Error Out Of Bag","Error clase 1","Error clase 0"),lty=c(1,1),lwd=c(2.5,2.5),col=c("black","green","red"))
```
\vspace{0.7cm}
Vemos que el error _Out Of Bag_ disminuye progresivamente hasta estabilizarse. Esto es así porque un modelo con pocos árboles adolece de sobreajuste, problema que se soluciona en parte al promediar las predicciones de un número creciente de clasificadores.

### _Boosting_

El siguiente tipo de modelo no lineal a estudiar es _Boosting_; en particular, la variante _AdaBoost_ utilizando funciones _stump_.

El concepto subyacente a _Boosting_ es combinar un conjunto predefinido de clasificadores débiles, entendiendo éstos como clasificadores cuya eficacia es marginalmente superior a uno aleatorio. Para constituir el modelo complejo final, se realiza iterativamente una agregación ponderada de dichos clasificadores en función de su $E_{in}$.

En nuestro caso, los clasificadores débiles que utilizaremos son denominados _stumps_, los cuales pueden ser vistos como árboles de decisión de un único nivel. La ponderación de cada _stump_ $t$ viene dada por el valor de su error $\epsilon_t$, calculado en base a una distribución $D_t$. Esta distribución confiere una importancia a cada observación, de forma que aquellas que han sido mal clasificadas durante más iteraciones tienen preferencia en las siguientes.

Para la implementación vamos a utilizar de nuevo \texttt{train}; en este caso con el método \texttt{ada}. Este método tiene tres hiperparámetros: la profundidad de los árboles clasificadores simples (\texttt{maxdepth}); el número de iteraciones o, de forma equivalente, el número de clasificadores distintos (\texttt{iter}); el coeficiente de aprendizaje (\texttt{nu}).

Puesto que vamos a utilizar como clasificadores débiles _stumps_, la profundidad de los árboles estará fijada a 1. Para optimizar los restantes hiperparámetros emplearemos de validación cruzada. Los valores entre los que se van a escoger son:

```{r, echo=FALSE}
set.seed(111)
```

```{r}
grid = expand.grid(maxdepth=1, iter=seq(20,100,10), 
                   nu=c(0.15,0.2,0.25,0.3))
# La documentación del paquete ada indica que se deben especificar además los siguientes 
# parámetros usando rpart para la utilización de stumps.
rcontrol = rpart.control(maxdepth=1,cp=-1,minsplit=0)
```

Una vez establecidos los valores entre los que se van a elegir los hiperparámetros, pasamos a obtener el mejor modelo y estudiar su calidad. De igual forma que en la sección anterior utlizaremos el preprocesamiento proporcionado por \texttt{train}.

```{r,cache=TRUE,message=FALSE}
ada_fit = train(x = datos[indices_train,], y = as.factor(etiquetas[indices_train]),
                method = "ada", trControl = control, 
                preProcess = c("YeoJohnson","center","scale"),
                tuneGrid = grid, control = rcontrol)
```

Para cada una de las combinaciones de hiperparámetros \texttt{train} proporciona la precisión del modelo asociado. Veamos los resultados de precisión para todas las combinaciones posibles:

\vspace{0.7cm}
```{r,echo=FALSE,fig.align='center',fig.height=3.9,fig.width=5.8}
plot(ada_fit,pch=20,xlab="Número de iteraciones / árboles",ylab="Precisión de validación cruzada")
```
\vspace{0.7cm}

Se observa una tendencia al alza en la precisión de la validación cruzada conforme aumentamos el número de iteraciones (o clasificadores simples distintos).

\newpage

Para visualizar mejor los mejores resultados, en la siguiente gráfica se muestran los valores del coeficiente de aprendizaje frente a la mayor precisión posible, junto con el número de iteraciones que se necesitaron para obtener dicho resultado.

\vspace{0.7cm}
```{r, echo=FALSE,message=FALSE,fig.align='center',fig.height=3.6,fig.width=5.3}
ada_pred = predict(ada_fit, datos[-indices_train,])
error_ada = porcentaje_error(as.numeric(ada_pred), etiquetas[-indices_train])

arboles_nu = ada_fit$result[which(ada_fit$results[,1] == 0.15)[which.max(ada_fit$results[ada_fit$results[,1] == 0.15,4])],3]
arboles_nu = c(arboles_nu,ada_fit$result[which(ada_fit$results[,1] == 0.2)[which.max(ada_fit$results[ada_fit$results[,1] == 0.2,4])],3])
arboles_nu = c(arboles_nu,ada_fit$result[which(ada_fit$results[,1] == 0.25)[which.max(ada_fit$results[ada_fit$results[,1] == 0.25,4])],3])
arboles_nu = c(arboles_nu,ada_fit$result[which(ada_fit$results[,1] == 0.3)[which.max(ada_fit$results[ada_fit$results[,1] == 0.3,4])],3])
datos_nu = c(0.15, max(ada_fit$results[ada_fit$results[,1] == 0.15,4]))
datos_nu = rbind(datos_nu,c(0.2, max(ada_fit$results[ada_fit$results[,1] == 0.2,4])))
datos_nu = rbind(datos_nu,c(0.25, max(ada_fit$results[ada_fit$results[,1] == 0.25,4])))
datos_nu = rbind(datos_nu,c(0.3, max(ada_fit$results[ada_fit$results[,1] == 0.3,4])))

plot(datos_nu, pch = 20, ylab = "Precisión", xlab = "Coeficiente de aprendizaje", xlim = c(0.13,0.32),col = "blue", type = "o")
text(x = datos_nu[,1], y = datos_nu[,2] , labels = arboles_nu, cex = 0.7, pos = 2)
```
\vspace{0.7cm}

Como podemos observar, la mayor precisión se obtiene con un coeficiente de aprendizaje igual a 0.25 con 60 iteraciones; por ello, éste sería el modelo a elegir en principio.

### _Support Vector Machines_

El tercer modelo a estudiar será _SVM_. Frente a modelos lineales como el perceptrón o las regresiones, _SVM_ busca construir un separador con mayor poder de generalización; logra esto buscando un separador cuya separación (margen) respecto a los puntos más cercanos (vectores de soporte) sea máxima. El razonamiento subyacente se basa en que los nuevos datos que debamos clasificar serán presumiblemente similares a los de entrenamiento; por lo tanto, si nuestro clasificador se acerca demasiado a algunos puntos, hay cierta probabilidad de que una nueva observación cercana a ellos sea etiquetada de forma incorrecta.

Análogamente a los modelos lineales mencionados, _SVM_ tiene su propia forma de tratar con datos no linealmente separables: los _kernels_. Un _kernel_ es una función que determina la similitud entre dos observaciones de forma equivalente a medir la distancia entre ellas en un espacio diferente al original. Nosotros usaremos el _kernel RBF_ gaussiano, cuya expresión se muestra tras estas líneas, que tiene un hiperparámetro $\gamma$ a ajustar. En el paquete \texttt{kernlab} que hemos utilizado a través de \texttt{train} este hiperparámetro es llamado $\sigma$.

$$K(x,x^\prime) = e^{-\gamma||x-x^\prime||^2} $$

Además, ya que los datos en la mayoría de los casos no son linealmente separables, \texttt{kernlab} nos permite ajustar el parámetro _C_, que controla la complejidad del modelo de forma similar a la regularización; en este caso, valores altos significan hipótesis más elaboradas que minimicen el error $\xi$ de cada punto (distancia al margen de su clase correcta; $\xi = 0$ si es un vector de soporte o está bien clasificado más allá de su margen; $0 < \xi < 2$ si está dentro de alguno de los dos márgenes; $\xi \geq 2$ si está mal clasificado más allá del margen opuesto).

\newpage

Por medio de validación cruzada estimaremos los parámetros _C_ y $\sigma$ para el modelo que nos ocupa. Como en las secciones anteriores, se realizarán 10 particiones de validación. Los valores entre los que se elegirán se definen en la siguiente línea de código:

```{r, echo=FALSE}
set.seed(111)
```

```{r}
grid = expand.grid(C=seq(1,5,1), sigma=seq(0.02, 0.04, 0.005))
```

Una vez hecho esto, veamos la llamada correspondiente a la función \texttt{train}. El método a utilizar es \texttt{svmRadial}; el resto de los parámetros se han comentado en modelos anteriores.

```{r,cache=TRUE,warning=FALSE}
svm_fit = train(x = datos[indices_train,], y = as.factor(etiquetas[indices_train]),
                method = "svmRadial", trControl = control,
                preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
```

Observemos las diferentes precisiones obtenidas por todas las combinaciones de valores de $\sigma$ y _C_:

\vspace{0.7cm}
```{r,echo=FALSE}
plot(svm_fit,pch=20,ylab="Precisión")
```
\vspace{0.7cm}

\newpage

Finalmente, vamos a analizar la máxima precisión posible con cada valor de $\sigma$, junto con sus valores de _C_ asociados:

\vspace{0.7cm}
```{r,echo=FALSE,fig.align='center',fig.height=3.6,fig.width=5.3}
svm_pred = predict(svm_fit, datos[-indices_train,])
error_svm = porcentaje_error(as.numeric(svm_pred), etiquetas[-indices_train])

c_sigma = svm_fit$result[which(svm_fit$results[,2] == 0.02)[which.max(svm_fit$results[svm_fit$results[,2] == 0.02,3])],1]
c_sigma = c(c_sigma,svm_fit$result[which(svm_fit$results[,2] == 0.025)[which.max(svm_fit$results[svm_fit$results[,2] == 0.025,3])],1])
c_sigma = c(c_sigma,svm_fit$result[which(svm_fit$results[,2] == 0.03)[which.max(svm_fit$results[svm_fit$results[,2] == 0.03,3])],1])
c_sigma = c(c_sigma,svm_fit$result[which(svm_fit$results[,2] == 0.035)[which.max(svm_fit$results[svm_fit$results[,2] == 0.035,3])],1])
c_sigma = c(c_sigma,svm_fit$result[which(svm_fit$results[,2] == 0.04)[which.max(svm_fit$results[svm_fit$results[,2] == 0.04,3])],1])
datos_sigma = c(0.02, max(svm_fit$results[svm_fit$results[,2] == 0.02,3]))
datos_sigma = rbind(datos_sigma,c(0.025, max(svm_fit$results[svm_fit$results[,2] == 0.025,3])))
datos_sigma = rbind(datos_sigma,c(0.03, max(svm_fit$results[svm_fit$results[,2] == 0.03,3])))
datos_sigma = rbind(datos_sigma,c(0.035, max(svm_fit$results[svm_fit$results[,2] == 0.035,3])))
datos_sigma = rbind(datos_sigma,c(0.04, max(svm_fit$results[svm_fit$results[,2] == 0.04,3])))

plot(datos_sigma, pch = 20, ylab = "Precisión", xlab = "Sigma", xlim = c(0.018,0.042),col = "blue", type = "o")
text(x = datos_sigma[,1], y = datos_sigma[,2] , labels = c_sigma, cex = 0.7, pos = 2)
```
\vspace{0.7cm}

Como se puede apreciar en la gráfica anterior, la mayor precisión (alternativamente, el menor error) de validación cruzada se obtiene con $\sigma =$ 0.04 y $C = 5$. Esta combinación de hiperparámetros será, pues, la elegida.

### Redes Neuronales

En este último apartado sobre modelos no lineales vamos a hablar de las redes neuronales. Las redes neuronales son una generalización del perceptrón. Puesto que es un modelo muy potente, es también muy sensible al sobreajuste. Por consiguiente, dado que para nuestro problema una simple regresión logística ya consigue un error bajo, es probable que una red neuronal aporte complejidad innecesaria.

Las redes neuronales pueden presentar distintas arquitecturas según el número de capas ocultas y el número de neuronas de cada una de éstas. En nuestro caso, probaremos redes neuronales con una, dos o tres capas ocultas que contendrán neuronas en el rango [1,50]. El peso inicial de cada neurona se asigna aleatoriamente.

```{r,echo=FALSE}
set.seed(17)
```

```{r,eval=FALSE}
grid = expand.grid(layer1=c(seq(1,50,3),50), layer2=0, layer3=0)
nn_fit_una = train(x = datos[indices_train,], y = etiquetas[indices_train],
                   method = "neuralnet", linear.output=FALSE, trControl = control,
                   preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
grid = expand.grid(layer1=c(seq(3,50,5),50), layer2=seq(3,50,5), layer3=0)
nn_fit_dos = train(x = datos[indices_train,], y = etiquetas[indices_train],
                   method = "neuralnet", linear.output=FALSE, trControl = control, 
                   preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
grid = expand.grid(layer1=c(seq(1,50,10),50), layer2=seq(1,50,10), layer3=seq(1,50,10))
nn_fit_tres = train(x = datos[indices_train,], y = etiquetas[indices_train],
                    method = "neuralnet", linear.output=FALSE, trControl = control,
                    preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
```

```{r, echo=FALSE}
load(file = "entorno_completo.RData")
```


\newpage

Ahora representaremos de forma gráfica el resultado de la validación cruzada para una y dos capas:

\vspace{0.7cm}
```{r,echo=FALSE,fig.align='center',fig.height=3.6,fig.width=5.3}
plot(nn_fit_una, ylab= "Error de validación cruzada", xlab="Neuronas", pch = 20)
```
```{r,echo=FALSE,fig.align='center',fig.height=3.9,fig.width=5.8}
plot(nn_fit_dos, ylab= "Error de validación cruzada", xlab="Neuronas Capa 1", pch = 20)
```

Los mejores resultados obtenidos por cada una de las tres variantes se pueden ver en la siguiente tabla:

|   Número de capas     | Neuronas por capa | $E_{CV}$ |
|:---------------------:|:-----------------:|:--------:|
| Una capa              | 13                | 1.6667   |
| Dos capas             | 2, 8              | 5.4545   |
| Tres capas            | 11, 31, 20        | 1.5151   |

Guiándonos por los números, la elección parece ser una red neuronal de tres capas. Sin embargo, ya que el rendimiento de la redes neuronales depende mucho de su arquitectura y no es sencillo encontrar un equilibrio entre potencia de ajuste a los datos de entrenamiento y generalización, en esta ocasión hemos decidido comparar las tres redes neuronales con los otros tipos de modelos.

Por otra parte, los experimentos realizados en este proyecto nos indican que el coste en términos de tiempo es prohibitivo: calcular estas redes neuronales ha consumido más de 8 horas, frente a pocos minutos en los demás casos. Por ello, aun si observásemos mejores resultados que los de sus competidores, probablemente no compensaría el esfuerzo computacional añadido.

```{r, echo=FALSE, fig.align='center',fig.height=3.6,fig.width=5.3, eval =FALSE}
# Como apunte final, comentaremos que las redes neuronales se pueden representar gráficamente en \texttt{R} de forma bastante inmediata. La representación deja de ser ilustrativa cuando el número de neuronas es elevado, de modo que aquí mostraremos el modelo obtenido más sencillo:
plot.nnet(nn_fit_dos$finalModel)
```


# Selección del modelo final
## Comparativa

Tras haber estudiado individualmente los modelos, es el momento de decantarnos por uno de ellos.

En primer lugar vamos a ver los $E_{in}$ y $E_{test}$ conseguidos por cada uno:

|         Modelo                | $E_{in}$ |  $E_{test}$ |
|:-----------------------------:|:--------:|:-----------:|
| Regresión Logística           |  5.9091  | 4.5936      |
| Random Forest                 |     0    | 6.3604      |
| Boosting                      |  8.3333  | 6.7137      |
| Support Vector Machine        |  1.3636  | 8.8339      |
| Red neuronal de una capa      |  1.6667  | 7.7738      |
| Red neuronal de dos capas     |  5.4545  | 9.5406      |
| Red neuronal de tres capas    |  1.5151  | 8.4805      |

De esta tabla se extrae que la regresión logística es más que suficiente para predecir con alta fiabilidad nuevos datos; de hecho, parece realizar un ajuste más consistente que los modelos no lineales. Para confirmar esto, vamos a analizar también las curvas _ROC_ y el área debajo de las mismas. Este último dato puede verse en la siguiente tabla:

| Modelo                    | Area Under Curve |
|:-------------------------:|:----------------:|
| Regresión Logística       |   0.9914568345   | 
| Random Forest             |   0.9774930056   | 
| Boosting                  |   0.9792915667   | 
| Support Vector Machine    |   0.9732713829   |
| Red neuronal de una capa  |   0.9581834532   |
| Red neuronal de dos capas |   0.9460431655   |
| Red neuronal de tres capas|   0.9407973621   |



Comprobamos que, una vez más, la regresión logística obtiene el mejor valor. La interpretación es la siguiente: este modelo tiene mayor precisión cuando consideramos todos los posibles umbrales a partir de los cuales decide clasificar positivamente una observación. Hay que destacar que los valores obtenidos por los otros modelos indican también una alta calidad general, pero los dos criterios que hemos usado apuntan en la misma dirección: la regresión logística es el modelo a elegir.

Con el objetivo de ofrecer mayor claridad acerca del significado de los valores de la tabla anterior, a continuación se ve una representación de las curvas _ROC_ de todos los modelos:

\vspace{0.7cm}

```{r, echo=FALSE, fig.align='center'}

calcula_curva_roc = function(pred,truth){
    predob = prediction(pred,truth)
    area = performance(predob,"auc")
    curva = performance(predob,"tpr","fpr")
    list(curva=curva,area=area)
}

# Curva ROC regresión logística
modelo_glm = ajustes_glm_sin_pca[3,glm_sin_pca_min_error_index]
eval_glm = unlist(evaluar_modelo(modelo_glm,datos_procesados_sin_pca[-indices_train,]))
roc_glm = calcula_curva_roc(eval_glm,etiquetas[-indices_train])

area_roc_glm = roc_glm$area@y.values
# Curva ROC Random Forest
modelo_rf = ajustes_rf_cv[3,rf_cv_min_error_index]
eval_rf = predict(modelo_rf,datos[-indices_train,],type="prob")$rf[,2]
roc_rf = calcula_curva_roc(eval_rf,etiquetas[-indices_train])

area_roc_rf = roc_rf$area@y.values
# Curva ROC AdaBoost
modelo_ada = ada_fit
eval_ada = predict(modelo_ada,datos[-indices_train,],type="prob")[,2]
roc_ada = calcula_curva_roc(eval_ada,etiquetas[-indices_train])

area_roc_ada = roc_ada$area@y.values
# Curva ROC Support Vector Machines
modelo_svm = svm_fit
eval_svm = predict(modelo_svm,datos[-indices_train,],type="prob")[,2]
roc_svm = calcula_curva_roc(eval_svm,etiquetas[-indices_train])

area_roc_svm = roc_svm$area@y.values

# Curva ROC Redes Neuronales con una capa
modelo_nn_una = nn_fit_una
eval_nn_una = compute(modelo_nn_una$finalModel,datos[-indices_train,])
roc_nn_una = calcula_curva_roc(eval_nn_una$net.result,etiquetas[-indices_train])

area_roc_nn_una = roc_nn_una$area@y.values

# Curva ROC Redes Neuronales con dos capas
modelo_nn_dos= nn_fit_dos
eval_nn_dos = compute(modelo_nn_dos$finalModel,datos[-indices_train,])
roc_nn_dos = calcula_curva_roc(eval_nn_dos$net.result,etiquetas[-indices_train])

area_roc_nn_dos = roc_nn_dos$area@y.values

# Curva ROC Redes Neuronales con tres capas
modelo_nn_tres= nn_fit_tres
eval_nn_tres = compute(modelo_nn_tres$finalModel,datos[-indices_train,])
roc_nn_tres = calcula_curva_roc(eval_nn_tres$net.result,etiquetas[-indices_train])

area_roc_nn_tres = roc_nn_tres$area@y.values

layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
par(mai=c(0.7,0.4,0.4,0.4),pty="s")
plot(roc_glm$curva, main = "R. Log", col = "blue")
plot(roc_rf$curva, main = "Random Forest", col = "blue")
plot(roc_ada$curva, main = "AdaBoost", col = "blue")
plot(roc_svm$curva, main = "SVM", col = "blue")
```
\vspace{0.7cm}
```{r, echo=FALSE,fig.align='center'}
#layout(matrix(c(1,2,3), 2, 3, byrow = TRUE))
par(mai=c(0.53,0.5,0.4,0.4),mfrow=c(2,3),pty="s")
plot(roc_nn_una$curva, main = "NN 1 layer", col = "blue")
plot(roc_nn_dos$curva, main = "NN 2 layers", col = "blue")
plot(roc_nn_tres$curva, main = "NN 3 layers", col = "blue")
```


## Descripción del modelo elegido

El modelo final será, como hemos mencionado anteriormente, la regresión logística. A continuación, vamos a presentar las características que han intervenido en el ajuste final junto con sus pesos y los valores utilizados en los distintos métodos de preprocesamiento:
    
| Var.  |   Peso  | Centrado | Escalado | Yeo-Jonhson | Var.     | Peso      | Centrado | Escalado| Yeo-Jonhson |
|:-----:|:-------:|:--------:|:--------:|:------------------:|:--------:|:---------:|---------:|:-------:|:------------------:|
| FSW.1 |  3.5753 | 6.9361   | 1.4599   | 0.3273             | SSP.2    |  -5.1918  | 162.4274 | 49.1844 | 1.4974             |
| SSW.1 |  1.4204 | 4.3006   | 1.1066   | 0.2869             | SSW.2    |  -1.0284  | 4.6731   | 1.3173  | 0.3366             |
| BPC.1 |  1.8938 | 2.2502   | 1.1279   | 0.3069             | BPC.2    |  -1.7250  | 2.2651   | 1.1205  | 0.3316             |
| BPW.1 |  0.9310 | 3.0814   | 1.5783   | 0.4139             | BPW.2    |  -0.8471  | 3.0681   | 1.5396  | 0.4159             |
| TPW.1 | 15.7692 | 7.9018   | 5.7159   | 0.3695             | NPW.2    |   0.2786  | 4.6566   | 2.4294  | 0.3841             |
| FSP.2 | -4.882 | 5.3850   | 0.2134   | 0.1221             | TPW.2    | -15.854  | 8.0051   | 5.7826  | 0.3730             |
| FSW.2 | -4.401 | 6.1526   | 1.1877   | 0.2717             | Intercept|   0.02848 |          |         |                    |

Para obtener un estimador del error fuera de la muestra vamos a emplear el error de test y la desigualdad de _Hoeffding_:
    
$$\mathbb{P}[|E_{test}(g) - E_{out}(g)| > \epsilon] \leq 2e^{-2N\epsilon^2}$$
    
Donde $N$ es el número de observaciones de la partición de test y $\mathbb{P}[|E_{test}(g) - E_{out}(g)| > \epsilon]$ corresponde a una tolerancia $\delta$ que fijaremos a 0.05.

$$\delta \leq 2e^{-2N\epsilon^2}$$
    
Junto con el error en la partición de test obtendremos, despejando $\epsilon$ de la siguiente forma, un intervalo en el que se encuentra $E_{out}$ con una tolerancia $\delta =$ 0.05.

$$ \sqrt{\frac{log(2)-log(\delta)}{2N}} \geq \epsilon$$
    
