---
lang: es
date: "1 de junio de 2017"
output: 
    pdf_document:
        fig_caption: yes
        toc: true
        number_sections: yes
        highlight: pygments
        includes:
            in_header: header.tex
            before_body: titlepage.tex
#bibliography: citas.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

```{r,echo=FALSE,message=FALSE}
library("neuralnet", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("caret", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("e1071", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("glmnet", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("ROCR", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("leaps", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("randomForest", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("ada", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
library("kernlab", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.3")
```

```{r,echo=FALSE}
set.seed(3)

leer_datos_partidos = function(){

    datos = read.csv("./datos/partidos.csv", header = TRUE)
    datos = datos[,-c(1,2,3)]
    # Añadimos tipo de pista como característica
    pista_dura = c(rep(1,253),rep(0,252),rep(1,202),rep(0,236))
    tierra_batida = c(rep(0,253),rep(1,252),rep(0,438))
    datos = cbind(datos,pista_dura,tierra_batida)
    etiquetas = datos[,1]
    indices = sample(nrow(datos),round(0.7*nrow(datos)))
    list(datos=datos[,-1],etiquetas=etiquetas,indices_train=indices)
    
}

# Preprocesamiento (centrado, escalado, análisis de componentes principales...)
preprocesar_datos = function(datos,indices_train,metodos,umbral_varianza=0.9){
    
    preprocess_obj = preProcess(datos[indices_train,],method=metodos,umbral_varianza)
    nuevosDatos = predict(preprocess_obj,datos)
    
}

# Evalúa la regresión para unos datos
evaluar_modelo = function(regresion,datos){
    
    predict(regresion,datos) # Los datos no deben incluir las etiquetas
    
}

porcentaje_error = function(clasificados,reales,fp=1,fn=1){
    
    reales[reales == 0] = -1
    t = table(clasificados,reales)
    total_predicciones = sum(t)
    t[1,2] = t[1,2]*fn
    t[2,1] = t[2,1]*fp
    100*(1-sum(diag(t))/total_predicciones)
    
}

categorizar = function(clasificados,umbral=0.5){
    
    clasificados[clasificados < umbral] = -1
    clasificados[clasificados >= umbral] = 1
    clasificados
    
}

# Evalúa una regresión lineal dada una fórmula y unos datos de entrenamiento 
evalua_lm = function(formula,datos,subconjunto,fp=1,fn=1){
    reg_lin = do.call("lm", list(formula=formula, data=substitute(datos), subset=substitute(subconjunto)))
    prediccion_test = evaluar_modelo(reg_lin,datos[-subconjunto,-ncol(datos)])
    porc_error = porcentaje_error(categorizar(prediccion_test),datos[-subconjunto,ncol(datos)],fp,fn)
    list(formula=formula, error = porc_error)
}

evalua_glm = function(formula,datos,subconjunto,fp=1,fn=1,familia=binomial()){
    reg_lin = do.call("glm", list(formula=formula, data=substitute(datos), subset=substitute(subconjunto),familia))
    prediccion_test = evaluar_modelo(reg_lin,datos[-subconjunto,-ncol(datos)])
    porc_error = porcentaje_error(categorizar(prediccion_test),datos[-subconjunto,ncol(datos)],fp,fn)
    list(formula=formula, error=porc_error,reg=reg_lin)
}
```

# Introducción

Para la realización de este trabajo se ha empleado la base de datos _Tennis Major Tournament Match Statistics_, que recoge información acerca de los partidos disputados en los _Grand Slam_ de 2013 (excepto por el Abierto de Australia, que por alguna razón es el de 2014). Los campos aportados por cada partido son los siguientes:

\begin{itemize}

    \item
    Nombres de los participantes.
    \item
    Resultado (desde el punto de vista del primer participante).
    \item
    Ronda.
    \item
    Porcentajes relativos al primer servicio de cada jugador: jugados y ganados.
    \item
    Porcentajes relativos al segundo servicio de cada jugador: jugados y ganados.
    \item
    Número de \textit{aces} por cada jugador.
    \item
    Número de dobles faltas cometidas por cada jugador.
    \item
    Número de puntos ganadores por cada jugador.
    \item
    Número de errores no forzados cometidos por cada jugador.
    \item
    Número de puntos de \textit{break} creados y ganados por cada jugador.
    \item
    Número de puntos en la red intentados y ganados por cada jugador.
    \item
    Puntos totales ganados por cada jugador.
    \item
    Juegos por set de cada jugador.
    \item
    Número total de sets ganados por cada jugador.
    
\end{itemize}

Esta base de datos contiene valores perdidos. No obstante, todos ellos representan cantidades que se pueden sustituir por cero de forma segura ya que denotan, por ejemplo, ausencia de errores no forzados, ausencia de _aces_ o sets no jugados (por abandono o por no ser necesarios).

Debido a que este conjunto de datos no tiene una variable de respuesta definida explícitamente, vamos a elegir una: el resultado del partido. Para que la predicción no sea trivial, es necesario eliminar algunas características que permiten determinar unívocamente el resultado, como el número de sets ganados por cada jugador y la puntuación dentro de los mismos. Además, por la naturaleza del dominio del problema, aprovecharemos que se nos proporciona implícitamente el tipo de pista en el que se han jugado los partidos para considerarlo como una característica más. Una vez realizado todo esto, comprobaremos así si es factible utilizar el resto de los predictores con el mismo propósito.

Un aspecto a tener en cuenta es el sesgo del propio conjunto de datos, provocado por la forma de construir las observaciones. Al ser la asignación de *Jugador 1* y *Jugador 2* arbitraria, de forma implícita se está favoreciendo que el mismo estadístico para un jugador tenga más importancia que para otro. Nosotros, por conocimiento general de este problema, sabemos que no importa en qué orden los nombremos; sin embargo, los modelos no tienen esta información y pueden llegar a conclusiones sesgadas basadas en este orden. Por ejemplo, para un mismo partido, la observación en la que el primer jugador comete 10 dobles faltas y el segundo comete 5 es equivalente a la observación en la que el primer jugador comete 5 dobles faltas y el segundo 10, ya que los jugadores son intercambiables. Un modelo no tiene esta información intuitiva y, por tanto, podría dar importancias distintas a variables en teoría simétricas.

En cuanto a la división en conjuntos de entrenamiento y test, no se nos proporciona un criterio de antemano, por lo que en principio la realizaremos nosotros tomando observaciones aleatorias según la proporción 70\%-30\%.

\newpage

# Preprocesamiento de datos.

Para realizar el preprocesamiento de datos hemos empleado la función \texttt{preProcess} del paquete \texttt{caret}; esta función permite aplicar distintos métodos de transformación para adecuar las características iniciales a los modelos que vamos a emplear.

A continuación vamos a explicar los métodos utilizados:

\begin{itemize}

    \item
    Transformación de \textit{Yeo-Johnson}: es muy similar a la transformación de \textit{Box-Cox} pero, a diferencia de ésta, permite la existencia de valores negativos ó 0. En nuestro caso, no existen valores negativos pero sí es bastante frecuente encontrar valores iguales a 0.
    \item
    Centrado: se realiza la media de los valores de cada característica y se le resta a cada valor particular, siendo la nueva media igual a 0. Esto reduce las distancias entre las distintas características.
    \item
    Escalado: este método divide los valores por su desviación típica. El objetivo de esta transformación es tener características con rangos uniformes para evitar problemas con modelos que consideran distancias entre valores (lo cual favorecería las características con valores más separados entre sí); además, la independencia respecto a la escala en la que se realizaron las medidas también es importante.
    \item
    Análisis de componentes principales (\textit{PCA}): se buscan nuevos ejes de coordenadas de forma que la varianza de algunas características sea lo suficientemente pequeña como para descartarlas. Como resultado se consigue una reducción de dimensionalidad donde las nuevas características, que no guardan relación de significado con las originales, representarán la parte más significativa de la varianza de los datos.

\end{itemize}

Otro método que se podría haber utilizado para reducir la dimensionalidad de los datos es _Near Zero Variance_, que elimina predictores con varianza cercana a 0, ya que esto suele indicar que su valor es casi constante a lo largo de la muestra y apenas aportan información. Sin embargo, hemos comprobado empíricamente que no afecta a nuestro conjunto de datos en particular.

Una vez explicadas las transformaciones, veamos cómo se traduce en la práctica a código:

```{r}
# Lectura de datos y obtención del conjunto de entrenamiento
datos = leer_datos_partidos()
etiquetas = datos$etiquetas
indices_train = datos$indices_train
datos = datos$datos

preprocesar_datos = function(datos,indices_train,metodos,umbral_varianza=0.9){
    
    preprocess_obj = preProcess(datos[indices_train,],method=metodos,umbral_varianza)
    nuevosDatos = predict(preprocess_obj,datos)
    
}

datos[is.na(datos)] = 0 # Sustituimos los valores perdidos por 0
datos = subset(datos,select=-c(FNL1,FNL2)) # Número de sets ganados por cada uno
datos = subset(datos,select=-c(ST1.1,ST2.1,ST3.1,ST4.1,ST5.1,ST1.2,ST2.2,ST3.2,ST4.2,ST5.2))
datos_procesados = preprocesar_datos(datos,indices_train,
                                     c("YeoJohnson","center","scale","pca"),0.85)
datos_procesados_sin_pca = preprocesar_datos(datos,indices_train,
                                             c("YeoJohnson","center","scale"))
```
```{r,echo=FALSE}
# Añadir las etiquetas para la regresión lineal
datos_procesados = cbind(datos_procesados,etiquetas)
datos_procesados_sin_pca = cbind(datos_procesados_sin_pca,etiquetas)
colnames(datos_procesados)[ncol(datos_procesados)] = "etiquetas"
colnames(datos_procesados_sin_pca)[ncol(datos_procesados_sin_pca)] = "etiquetas"
```

Ya que existe la posibilidad de que el análisis de componentes principales elimine características relevantes, vamos a realizar experimentos separados preprocesando los datos con y sin esta técnica.

# Modelos a estudiar

En principio, podemos comenzar comprobando la calidad de un modelo lineal; tras ello, fuese o no necesario (debido a la naturaleza de este proyecto), probaremos modelos no lineales como _Random Forest_, _Boosting_, _Support Vector Machines_ o redes neuronales.

## Modelos lineales

Ya que el objetivo de este proyecto es analizar la eficacia de modelos no lineales, elegiremos un único modelo lineal de forma no tan exhaustiva como en el trabajo 3.

En primer lugar, vamos a formarnos una idea de la utilidad en este contexto de ambos conjuntos de datos preprocesados guiándonos por la tasa de error que produce una regresión logística con todas sus características:

```{r}
reg_log = evalua_glm(etiquetas~.,datos_procesados,indices_train)
reg_log_sin_pca = evalua_glm(etiquetas~.,datos_procesados_sin_pca,indices_train)
```

El primer ajuste da un error de 8.83\%, frente al segundo, que da un 5.3\%. Según este criterio, elegiremos el segundo modelo; no obstante, ya que utiliza todas las características a su disposición, vamos a tratar de encontrar un subconjunto de ellas con la función \texttt{regsubsets} que mantenga la calidad a la vez que ofrece una reducción de dimensionalidad.

Dicha función es empleada de la siguiente forma para obtener los subconjuntos de variables con los que vamos a calcular las distintas regresiones:

```{r}
subconjuntos_formulas = function(datos,max_tam,metodo="exhaustive"){
    # Obtenemos los subconjuntos de variables
    subsets = regsubsets(etiquetas~.,data=datos,method=metodo,nvmax=max_tam)
    # Obtenemos la matriz de características seleccionadas por grupos de tamaño [1,nvmax]
    matriz_subsets = summary(subsets)$which[,-1]
    # Guardamos, para cada fila, las columnas cuyas variables han sido seleccionadas.
    seleccionados = apply(matriz_subsets,1,which)
    # Obtenemos los nombres de esas columnas (para utilizarlos en la regresión)
    seleccionados = lapply(seleccionados,names)
    # Construimos la suma de las variables que usaremos en la regresión lineal
    seleccionados = mapply(paste,seleccionados,MoreArgs=list(collapse="+"))
    # Construimos strings equivalentes a las fórmulas que usaremos en la regresión lineal
    formulas = mapply(paste,rep("etiquetas~",max_tam),seleccionados,USE.NAMES = FALSE)
    # Construimos objetos fórmula
    formulas = apply(matrix(formulas,nrow=length(formulas)), 1, as.formula)
    list(formulas=formulas,cp=summary(subsets)$cp,bic=summary(subsets)$bic)
}
```

Un ejemplo de uso es:

```{r}
# Seleccionamos subconjuntos de características para los datos con PCA
max_caracteristicas = ncol(datos_procesados)-1
seleccion_caracteristicas = subconjuntos_formulas(datos_procesados[indices_train,],
                                                  max_caracteristicas,metodo="exhaustive")
formulas = seleccion_caracteristicas$formulas
```

```{r,echo=FALSE}
# Seleccionamos subconjuntos de características para los datos sin PCA
max_caracteristicas_sin_pca = ncol(datos_procesados_sin_pca)-1
seleccion_caracteristicas_sin_pca = subconjuntos_formulas(datos_procesados_sin_pca[indices_train,],max_caracteristicas_sin_pca,metodo="exhaustive")
formulas_sin_pca = seleccion_caracteristicas_sin_pca$formulas
```

\newpage

Ahora realizaremos ajustes para todos los subconjuntos de características obtenidos mediante el código anterior:

```{r}
ajustes_glm = mapply(evalua_glm, formulas, 
                     MoreArgs = list(datos = datos_procesados, 
                                     subconjunto = indices_train))
ajustes_glm_sin_pca = mapply(evalua_glm, formulas_sin_pca, 
                             MoreArgs = list(datos = datos_procesados_sin_pca, 
                                             subconjunto = indices_train))
```

Para comparar más fácilmente los porcentajes de error obtenidos por los modelos, vamos a representarlos en la siguiente gráfica:

\vspace{0.7cm}

```{r,echo=FALSE,,fig.align='center'}
glm_min_error_index = which.min(unlist(ajustes_glm[2,]))
glm_sin_pca_min_error_index = which.min(unlist(ajustes_glm_sin_pca[2,]))
plot(x=1:ncol(ajustes_glm_sin_pca),y=ajustes_glm_sin_pca[2,],pch=20,ylim=c(4,30),type="o",col="blue",xlab="Tamaño del conjunto",ylab="Error porcentual", main = "Comparativa de regresiones logísticas")
points(x=glm_sin_pca_min_error_index, y=ajustes_glm_sin_pca[2,glm_sin_pca_min_error_index], pch=19, col="orange")
points(x=1:ncol(ajustes_glm),y=ajustes_glm[2,],type="o",pch=20,col="red")
points(x=glm_min_error_index, y=ajustes_glm[2,glm_min_error_index],pch=19,col="green")
legend(16.5,29,c("Sin PCA","Con PCA"),lty=c(1,1),lwd=c(2.5,2.5),col=c("blue","red"))
```

\vspace{0.7cm}

El punto señalado en verde nos indica el conjunto de predictores cuyo error ha sido menor (8.48\%) para los datos con análisis de componentes principales. Análogamente, el punto naranja señala el conjunto de características que ha dado menor error (4.59\%) para los datos sin PCA.

Ya que hemos conseguido no sólo reducir el número de predictores necesarios (de 28 a 13) sino también el error, vamos a elegir el modelo que proporciona un 4.59\% de error para futuras comparaciones con modelos no lineales.

\newpage

## Modelos no lineales

Procediendo de forma similar, analizaremos varios modelos no lineales utilizando los dos conjuntos de características obtenidos en la sección de preprocesamiento de datos. Antes de empezar, es preciso apuntar que los ajustes lineales anteriores han obtenido errores bastante bajos, por lo que la complejidad adicional introducida por las técnicas que veremos a continuación deberá tener asociado un error aún más bajo que la justifique.

Para automatizar la optimización de hiperparámetros mediante validación cruzada utilizaremos las funcionalidades proporcionadas por el paquete \texttt{caret}; en concreto, la función \texttt{train}. Esto se explorará con mayor profundidad en los apartados dedicados a cada tipo de modelo.

### _Random Forest_

```{r,echo=FALSE}
evalua_random_forest_cv = function(datos,etiquetas,arboles=100,k=10,i=1,metodo="cv"){
    mtry = sqrt(ncol(datos))
    rf_train_control = trainControl(method=metodo,number=k,repeats=i)
    rf_train = train(datos,as.factor(etiquetas),method="rf",ntree=arboles,preProcess=c("YeoJohnson","center","scale"),trControl=rf_train_control,tuneGrid=expand.grid(.mtry=mtry))
    rf_error = (1-rf_train$results$Accuracy)*100
    list(arboles=arboles,error=rf_error,rf=rf_train)
}
set.seed(111)
```

El primer modelo no lineal que vamos a estudiar será _Random Forest_. Esta técnica consiste en
entrenar un cierto número de árboles usando subconjuntos aleatorios de características para después promediar sus resultados.

Partiendo del concepto de _bagging_, _Random Forest_ muestrea con reemplazo el conjunto de $n$ datos original para obtener $p$ conjuntos de un cierto tamaño $n'$ con los que ajustar $p$ árboles. Lo que diferencia estas dos técnicas es la elección de subconjuntos aleatorios de variables utilizadas en el ajuste de cada árbol que realiza _Random Forest_.

Como hemos comentado, vamos a utilizar \texttt{train} para encontrar, en este caso, el número de árboles (\texttt{ntree}) más adecuado para una cantidad de variables por árbol fija.

Para empezar, vamos a especificar que queremos realizar validación cruzada. Esto se hace creando un objeto \texttt{trainControl} que posteriormente \texttt{train} tomará como argumento.

```{r}
control = trainControl(method = "cv", number = 10) # 10-fold cv
```

El siguiente paso es crear un conjunto de posibles valores de \texttt{ntree}:

```{r}
num_arboles = seq(50,500,5)
```

Utilizaremos la función que se muestra a continuación para obtener el error de validación cruzada para cada uno de esos valores:

```{r ajustes_rf_cv, fig.align='center', warning=FALSE, cache=TRUE}
evalua_random_forest_cv = function(datos,etiquetas,control,arboles=100){
    mtry = sqrt(ncol(datos))
    rf_train = train(datos,as.factor(etiquetas),
                     method="rf",ntree=arboles,
                     preProcess=c("YeoJohnson","center","scale"),
                     trControl=control,tuneGrid=expand.grid(.mtry=mtry))
    rf_error = (1-rf_train$results$Accuracy)*100
    list(arboles=arboles,error=rf_error,rf=rf_train)
}

ajustes_rf_cv = mapply(evalua_random_forest_cv,num_arboles,
                       MoreArgs = list(datos=datos[indices_train,],
                                       etiquetas=etiquetas[indices_train],
                                       control=control))
```

Siguiendo las directrices del proyecto, mantendremos constante el número de variables predictoras a $\sqrt{n}$, donde $n$ es el total de características disponibles.

Los parámetros principales que recibe \texttt{train} son, por este orden: los datos originales, las etiquetas, el método (en nuestro caso, _Random Forest_), el número de árboles específico y las transformaciones de preprocesamiento. Además, hemos de pasarle el objeto \texttt{control} y el número de variables definidos previamente. Finalmente, debemos tener en cuenta otros parámetros que no aparecen en el código por tener valores correctos por defecto: \texttt{replace} (muestreo con o sin reemplazo) o \texttt{sampsize} ($n'$, tamaño de los conjuntos obtenidos).

Ya que vamos a realizar validación cruzada, es importante no contaminar el conjunto de datos realizando el preprocesamiento antes de crear las particiones de validación. La razón de esto es que esta técnica toma información de todos los datos a su disposición para homogeneizar los valores de las características, por lo que si preprocesamos el conjunto completo las particiones de validación no serán totalmente independientes. Es por esto que \texttt{train} aplica las transformaciones en cada iteración de la validación cruzada.

Una vez obtenidos los $E_{CV}$, decidiremos qué número de árboles es el más adecuado entre los candidatos según este criterio. Esta decisión se ilustra mejor con la siguiente gráfica:

\vspace{0.7cm}
```{r,echo=FALSE}
rf_cv_min_error_index = which.min(unlist(ajustes_rf_cv[2,]))

plot(x=num_arboles,y=ajustes_rf_cv[2,],pch=20,ylim=c(4,19),type="o",col="blue",xlab="Número de árboles",ylab="% Error de validación cruzada", main = "Comparativa de número de árboles")
points(x=ajustes_rf_cv[1,rf_cv_min_error_index], y=ajustes_rf_cv[2,rf_cv_min_error_index], pch=19, col="orange")
```
\vspace{0.7cm}

El punto señalado en naranja indica el número de árboles con el que se ha obtenido el menor $E_{CV}$: 465. Por ello, ajustaremos un modelo con 465 árboles utilizando toda la partición de entrenamiento para posteriormente evaluar su calidad sobre la partición de test.

Para finalizar, veamos cómo ha evolucionado el error de este modelo conforme aumenta su número de árboles:

\vspace{0.7cm}
```{r,echo=FALSE,fig.align='center'}
plot(ajustes_rf_cv[3,rf_cv_min_error_index]$rf$finalModel,lty=1,main="Evolución del mejor Random Forest")
legend(290,0.25,c("Error Out Of Bag","Error clase 1","Error clase 0"),lty=c(1,1),lwd=c(2.5,2.5),col=c("black","green","red"))
```
\vspace{0.7cm}

### _Boosting_

El siguiente tipo de modelo no lineal a estudiar es _Boosting_; en particular, la variante _AdaBoost_ utilizando funciones _stump_.

El concepto subyacente a _Boosting_ es combinar un conjunto predefinido de clasificadores débiles, entendiendo éstos como clasificadores cuya eficacia es marginalmente superior a uno aleatorio. Para constituir el modelo complejo final, se realiza iterativamente una agregación ponderada de dichos clasificadores en función de su $E_{in}$.

En nuestro caso, los clasificadores débiles que utilizaremos son denominados _stumps_, los cuales pueden ser vistos como árboles de decisión de un único nivel. La ponderación de cada _stump_ $t$ viene dada por el valor de su error $\epsilon_t$, calculado en base a una distribución $D_t$. Esta distribución confiere una importancia a cada observación, de forma que aquellas que han sido mal clasificadas durante más iteraciones tienen preferencia en las siguientes.

Para la implementación vamos a utilizar de nuevo \texttt{train}; en este caso con el método \texttt{ada}. Este método tiene tres hiperparámetros: la profundidad de los árboles clasificadores simples (\texttt{maxdepth}); el número de iteraciones o, de forma equivalente, el número de clasificadores distintos (\texttt{iter}); el coeficiente de aprendizaje (\texttt{nu}).

Puesto que vamos a utilizar como clasificadores débiles _stumps_, la profundidad de los árboles estará fijada a 1. Para optimizar los restantes hiperparámetros emplearemos de validación cruzada. Los valores entre los que se van a escoger son:

```{r}
set.seed(111)
grid = expand.grid(maxdepth=1, iter=seq(20,100,10), 
                   nu=c(0.15,0.2,0.25,0.3))
# La documentación del paquete ada indica que se deben especificar además los siguientes 
# parámetros usando rpart para la utilización de stumps.
rcontrol = rpart.control(maxdepth=1,cp=-1,minsplit=0)
```

Una vez establecidos los valores entre los que se van a elegir los hiperparámetros, pasamos a obtener el mejor modelo y estudiar su calidad. De igual forma que en la sección anterior utlizaremos el preprocesamiento proporcionado por \texttt{train}.

```{r,cache=TRUE,message=FALSE}
ada_fit = train(x = datos[indices_train,], y = as.factor(etiquetas[indices_train]),
                method = "ada", trControl = control, 
                preProcess = c("YeoJohnson","center","scale"),
                tuneGrid = grid, control = rcontrol)
```

Para cada una de las combinaciones de hiperparámetros \texttt{train} proporciona la precisión del modelo asociado. Veamos los resultados de precisión para todas las combinaciones posibles:

\vspace{0.7cm}
```{r,echo=FALSE,fig.align='center',fig.height=3.9,fig.width=5.8}
plot(ada_fit,pch=20,xlab="Número de iteraciones / árboles",ylab="Precisión de validación cruzada")
```
\vspace{0.7cm}

Se observa una tendencia al alza en la precisión de la validación cruzada conforme aumentamos el número de iteraciones (o clasificadores simples distintos).

Para visualizar mejor los mejores resultados, en la siguiente gráfica se muestran los valores del coeficiente de aprendizaje frente a la mayor precisión posible, junto con el número de iteraciones que se necesitaron para obtener dicho resultado.

\vspace{0.7cm}
```{r, echo=FALSE,message=FALSE,fig.align='center',fig.height=3.6,fig.width=5.3}
ada_pred = predict(ada_fit, datos[-indices_train,])
error_ada = porcentaje_error(as.numeric(ada_pred), etiquetas[-indices_train])

arboles_nu = ada_fit$result[which(ada_fit$results[,1] == 0.15)[which.max(ada_fit$results[ada_fit$results[,1] == 0.15,4])],3]
arboles_nu = c(arboles_nu,ada_fit$result[which(ada_fit$results[,1] == 0.2)[which.max(ada_fit$results[ada_fit$results[,1] == 0.2,4])],3])
arboles_nu = c(arboles_nu,ada_fit$result[which(ada_fit$results[,1] == 0.25)[which.max(ada_fit$results[ada_fit$results[,1] == 0.25,4])],3])
arboles_nu = c(arboles_nu,ada_fit$result[which(ada_fit$results[,1] == 0.3)[which.max(ada_fit$results[ada_fit$results[,1] == 0.3,4])],3])
datos_nu = c(0.15, max(ada_fit$results[ada_fit$results[,1] == 0.15,4]))
datos_nu = rbind(datos_nu,c(0.2, max(ada_fit$results[ada_fit$results[,1] == 0.2,4])))
datos_nu = rbind(datos_nu,c(0.25, max(ada_fit$results[ada_fit$results[,1] == 0.25,4])))
datos_nu = rbind(datos_nu,c(0.3, max(ada_fit$results[ada_fit$results[,1] == 0.3,4])))

plot(datos_nu, pch = 20, ylab = "Precisión", xlab = "Coeficiente de aprendizaje", xlim = c(0.13,0.32),col = "blue", type = "o")
text(x = datos_nu[,1], y = datos_nu[,2] , labels = arboles_nu, cex = 0.7, pos = 2)
```
\vspace{0.7cm}

Como podemos observar, la mayor precisión se obtiene con un coeficiente de aprendizaje igual a 0.25 con 60 iteraciones; por ello, éste sería el modelo a elegir en principio.

### _Support Vector Machines_

El tercer modelo a estudiar será _SVM_. Frente a modelos lineales como el perceptrón o las regresiones, _SVM_ busca construir un separador con mayor poder de generalización; logra esto buscando un separador cuya separación (margen) respecto a los puntos más cercanos (vectores de soporte) sea máxima. El razonamiento subyacente se basa en que los nuevos datos que debamos clasificar serán presumiblemente similares a los de entrenamiento; por lo tanto, si nuestro clasificador se acerca demasiado a algunos puntos, hay cierta probabilidad de que una nueva observación cercana a ellos sea etiquetada de forma incorrecta.

Análogamente a los modelos lineales mencionados, _SVM_ tiene su propia forma de tratar con datos no linealmente separables: los _kernels_. Un _kernel_ es una función que determina la similitud entre dos observaciones de forma equivalente a medir la distancia entre ellas en un espacio diferente al original. Nosotros usaremos el _kernel RBF_ gaussiano, que tiene un hiperparámetro $\gamma$ a ajustar. En el paquete \texttt{kernlab} que hemos utilizado a través de \texttt{train} este hiperparámetro es llamado $\sigma$ [@kernlab].

Además, ya que los datos en la mayoría de los casos no son linealmente separables, \texttt{kernlab} nos permite ajustar el parámetro _C_, que controla la complejidad del modelo de forma similar a la regularización; en este caso, valores altos significan hipótesis más elaboradas que minimicen el error $\xi$ de cada punto (distancia al margen de su clase correcta; $\xi = 0$ si es un vector de soporte o está bien clasificado más allá de su margen; $0 < \xi < 2$ si está dentro de alguno de los dos márgenes; $\xi \geq 2$ si está mal clasificado más allá del margen opuesto).

Por medio de validación cruzada estimaremos los parámetros _C_ y $\sigma$ para el modelo que nos ocupa. Como en las secciones anteriores, se realizarán 10 particiones de validación. Los valores entre los que se elegirán se definen en la siguiente línea de código:

```{r, echo=FALSE}
set.seed(111)
```

```{r}
grid = expand.grid(C=seq(1,5,1), sigma=seq(0.02, 0.04, 0.005))
```

Una vez hecho esto, veamos la llamada correspondiente a la función \texttt{train}. El método a utilizar es \texttt{svmRadial}; el resto de los parámetros se han comentado en modelos anteriores.

```{r,cache=TRUE,warning=FALSE}
svm_fit = train(x = datos[indices_train,], y = as.factor(etiquetas[indices_train]),
                method = "svmRadial", trControl = control,
                preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
```

Observemos las diferentes precisiones obtenidas por todas las combinaciones de valores de $\sigma$ y _C_:

\vspace{0.7cm}
```{r,echo=FALSE}
plot(svm_fit,pch=20,ylab="Precisión")
```
\vspace{0.7cm}

Finalmente, vamos a analizar la máxima precisión posible con cada valor de $\sigma$, junto con sus valores de _C_ asociados:

\vspace{0.7cm}
```{r,echo=FALSE,fig.align='center',fig.height=3.6,fig.width=5.3}
svm_pred = predict(svm_fit, datos[-indices_train,])
error_svm = porcentaje_error(as.numeric(svm_pred), etiquetas[-indices_train])

c_sigma = svm_fit$result[which(svm_fit$results[,2] == 0.02)[which.max(svm_fit$results[svm_fit$results[,2] == 0.02,3])],1]
c_sigma = c(c_sigma,svm_fit$result[which(svm_fit$results[,2] == 0.025)[which.max(svm_fit$results[svm_fit$results[,2] == 0.025,3])],1])
c_sigma = c(c_sigma,svm_fit$result[which(svm_fit$results[,2] == 0.03)[which.max(svm_fit$results[svm_fit$results[,2] == 0.03,3])],1])
c_sigma = c(c_sigma,svm_fit$result[which(svm_fit$results[,2] == 0.035)[which.max(svm_fit$results[svm_fit$results[,2] == 0.035,3])],1])
c_sigma = c(c_sigma,svm_fit$result[which(svm_fit$results[,2] == 0.04)[which.max(svm_fit$results[svm_fit$results[,2] == 0.04,3])],1])
datos_sigma = c(0.02, max(svm_fit$results[svm_fit$results[,2] == 0.02,3]))
datos_sigma = rbind(datos_sigma,c(0.025, max(svm_fit$results[svm_fit$results[,2] == 0.025,3])))
datos_sigma = rbind(datos_sigma,c(0.03, max(svm_fit$results[svm_fit$results[,2] == 0.03,3])))
datos_sigma = rbind(datos_sigma,c(0.035, max(svm_fit$results[svm_fit$results[,2] == 0.035,3])))
datos_sigma = rbind(datos_sigma,c(0.04, max(svm_fit$results[svm_fit$results[,2] == 0.04,3])))

plot(datos_sigma, pch = 20, ylab = "Precisión", xlab = "Sigma", xlim = c(0.018,0.042),col = "blue", type = "o")
text(x = datos_sigma[,1], y = datos_sigma[,2] , labels = c_sigma, cex = 0.7, pos = 2)
```
\vspace{0.7cm}

Como se puede apreciar en la gráfica anterior, la mayor precisión (alternativamente, el menor error) de validación cruzada se obtiene con $\sigma =$ 0.04 y $C = 5$. Esta combinación de hiperparámetros será, pues, la elegida.

### Redes Neuronales

```{r,echo=FALSE}
set.seed(17)
```

```{r,results=FALSE}
grid = expand.grid(layer1=c(seq(1,50,3),50), layer2=0, layer3=0)
nn_fit_una = train(x = datos[indices_train,], y = etiquetas[indices_train],
                   method = "neuralnet", linear.output=FALSE, trControl = control,
                   preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
grid = expand.grid(layer1=c(seq(3,50,5),50), layer2=seq(3,50,5), layer3=0)
nn_fit_dos = train(x = datos[indices_train,], y = etiquetas[indices_train],
                   method = "neuralnet", linear.output=FALSE, trControl = control, 
                   preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
grid = expand.grid(layer1=c(seq(1,50,10),50), layer2=seq(0,50,10), layer3=seq(0,50,10))
nn_fit_tres = train(x = datos[indices_train,], y = etiquetas[indices_train],
                    method = "neuralnet", linear.output=FALSE, trControl = control,
                    preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
```
```{r nn,echo=FALSE,cache=TRUE}
grid = expand.grid(layer1=c(seq(1,50,3),50), layer2=0, layer3=0)
nn_fit_una = train(x = datos[indices_train,], y = etiquetas[indices_train],method = "neuralnet", linear.output=FALSE, trControl = control, preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
grid = expand.grid(layer1=c(seq(3,50,5),50), layer2=seq(3,50,5), layer3=0)
nn_fit_dos = train(x = datos[indices_train,], y = etiquetas[indices_train],method = "neuralnet", linear.output=FALSE, trControl = control, preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
grid = expand.grid(layer1=c(seq(1,50,10),50), layer2=seq(0,50,10), layer3=seq(0,50,10))
nn_fit_tres = train(x = datos[indices_train,], y = etiquetas[indices_train],method = "neuralnet", linear.output=FALSE, trControl = control, preProcess = c("YeoJohnson","center","scale"), tuneGrid = grid)
```

## Selección del modelo final

Tras haber estudiado individualmente los modelos, es el momento de decantarnos por uno de ellos.

En primer lugar vamos a ver los $E_{in}$ y $E_{test}$ conseguidos por cada uno:

```{r,results=FALSE,echo=FALSE}
# Insertar tabla
```

De esta tabla se extrae que la regresión logística es suficiente para predecir con alta fiabilidad nuevos datos; de hecho, parece realizar un mejor ajuste que los modelos no lineales. Para confirmar esto, vamos a analizar también las curvas _ROC_ y el área debajo de las mismas.

| Modelo                 | Area Under Curve |
|:----------------------:|:----------------:|
| Regresión Logística    |   0.9914568345   | 
| Random Forest          |   0.9774930056   | 
| Boosting               |   0.9792915667   | 
| Support Vector Machine |   0.9732713829   |

\newpage

# Referencias